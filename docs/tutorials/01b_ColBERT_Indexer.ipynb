{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "764f905e",
   "metadata": {},
   "source": [
    "# Tutorial: Build a search index using DPR #\n",
    "\n",
    "In this tutorial, we will learn how to build a Neural Search index over your document collection. The algorithm displayed here is called Dense Passage Retrieval (DPR) as described in Karpukhin et al., \"Dense Passage Retrieval for Open-Domain Question Answering\" [here](https://arxiv.org/pdf/2004.04906.pdf).\n",
    "\n",
    "For the purposes of making this tutorial easy to understand we show the steps using a very small document collection. Note that this technique can be used to scale to millions of documents. We have tested upto 21 million Wikipedia passages!!!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4fca36ec",
   "metadata": {},
   "source": [
    "## Preparing a Colab Environment to run this tutorial ##\n",
    "\n",
    "Make sure to \"Enable GPU Runtime\" -> make a URL with a page with screenshots on how to do this."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5e3426d",
   "metadata": {},
   "source": [
    "## Installing PrimeQA\n",
    "\n",
    "First, we need to include the required modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c4d5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip install --upgrade pip\n",
    "pip install primeqa"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e289b31",
   "metadata": {},
   "source": [
    "## Pre-process your document collection here to be ready to be stored in your Neural Search Index.\n",
    "\n",
    "TODO- add some steps after this to ingest from the sample wikipedia docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c03fd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='/opt/share/cuda-11.1/x86_64'\n",
      "{\"time\":\"2023-06-04 19:53:51,723\", \"name\": \"faiss.loader\", \"level\": \"INFO\", \"message\": \"Loading faiss.\"}\n",
      "{\"time\":\"2023-06-04 19:53:51,741\", \"name\": \"faiss.loader\", \"level\": \"INFO\", \"message\": \"Successfully loaded faiss.\"}\n"
     ]
    }
   ],
   "source": [
    "from primeqa.components.indexer.dense import ColBERTIndexer "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "198cfd78",
   "metadata": {},
   "source": [
    "## Initializing the Indexer\n",
    "\n",
    "We initialize a ColBERT indexer which will be used for indexing the embeddings created for each document (passage) in the collection. It takes a passage_embedding_model to create the embedding vectors and a vector_db specification where it stores the embedding vectors to search later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "346c9aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#indexer= ColBERTIndexer (passage_embedding_model = \"/dccstor/colbert-ir/bsiyer/PQLL/experiments/xor_squad_04182023/2023-04/22/17.23.31/checkpoints/colbert.dnn.batch_17524.model\")\n",
    "#ToDO checkpoint to be renamed to passage_embedding_model\n",
    "indexer= ColBERTIndexer (checkpoint = \"/dccstor/colbert-ir/bsiyer/PQLL/experiments/xor_squad_04182023/2023-04/22/17.23.31/checkpoints/colbert.dnn.batch_17524.model\", vector_db='FAISS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c82a4580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexer: collection path set to: /dccstor/irl-tableqa/jaydeep/sample-document-store2.tsv\n",
      "\n",
      "\n",
      "[Jun 04, 19:54:35] #> Creating directory index_root/index_name \n",
      "\n",
      "\n",
      "#> Starting...\n",
      "No CUDA runtime is found, using CUDA_HOME='/opt/share/cuda-11.1/x86_64'\n",
      "{\"time\":\"2023-06-04 19:54:37,875\", \"name\": \"faiss.loader\", \"level\": \"INFO\", \"message\": \"Loading faiss.\"}\n",
      "{\"time\":\"2023-06-04 19:54:37,942\", \"name\": \"faiss.loader\", \"level\": \"INFO\", \"message\": \"Successfully loaded faiss.\"}\n",
      "{\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"index_path\": \"index_root\\/index_name\",\n",
      "    \"index_location\": null,\n",
      "    \"nbits\": 1,\n",
      "    \"kmeans_niters\": 4,\n",
      "    \"num_partitions_max\": 10000000,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 64,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 3e-6,\n",
      "    \"maxsteps\": 500000,\n",
      "    \"save_every\": null,\n",
      "    \"resume\": false,\n",
      "    \"resume_optimizer\": false,\n",
      "    \"warmup\": null,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 2,\n",
      "    \"use_ib_negatives\": false,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"shuffle_every_epoch\": false,\n",
      "    \"save_steps\": 2000,\n",
      "    \"save_epochs\": -1,\n",
      "    \"epochs\": 10,\n",
      "    \"input_arguments\": {},\n",
      "    \"model_type\": \"roberta-large\",\n",
      "    \"init_from_lm\": null,\n",
      "    \"local_models_repository\": null,\n",
      "    \"ranks_fn\": null,\n",
      "    \"output_dir\": null,\n",
      "    \"topK\": 100,\n",
      "    \"student_teacher_temperature\": 1.0,\n",
      "    \"student_teacher_top_loss_weight\": 0.5,\n",
      "    \"teacher_model_type\": \"xlm-roberta-base\",\n",
      "    \"teacher_doc_maxlen\": 180,\n",
      "    \"distill_query_passage_separately\": false,\n",
      "    \"query_only\": false,\n",
      "    \"loss_function\": null,\n",
      "    \"query_weight\": 0.5,\n",
      "    \"rng_seed\": 12345,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 128,\n",
      "    \"doc_maxlen\": 180,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"\\/dccstor\\/colbert-ir\\/bsiyer\\/PQLL\\/experiments\\/xor_squad_04182023\\/2023-04\\/22\\/17.23.31\\/checkpoints\\/colbert.dnn.batch_17524.model\",\n",
      "    \"teacher_checkpoint\": null,\n",
      "    \"triples\": null,\n",
      "    \"teacher_triples\": null,\n",
      "    \"collection\": \"\\/dccstor\\/irl-tableqa\\/jaydeep\\/sample-document-store2.tsv\",\n",
      "    \"queries\": null,\n",
      "    \"index_name\": \"index_name\",\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\\/dccstor\\/irl-tableqa\\/jaydeep\\/primeqa\\/docs\\/tutorials\\/experiments\",\n",
      "    \"experiment\": \"default\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2023-06\\/04\\/19.53.49\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 0\n",
      "}\n",
      "[Jun 04, 19:54:37] #> Loading collection...\n",
      "0M \n",
      "[Jun 04, 19:54:37] #>>>>> at ColBERT name (model type) : /dccstor/colbert-ir/bsiyer/PQLL/experiments/xor_squad_04182023/2023-04/22/17.23.31/checkpoints/colbert.dnn.batch_17524.model\n",
      "[Jun 04, 19:54:37] #>>>>> at BaseColBERT name (model type) : /dccstor/colbert-ir/bsiyer/PQLL/experiments/xor_squad_04182023/2023-04/22/17.23.31/checkpoints/colbert.dnn.batch_17524.model\n",
      "[Jun 04, 19:54:42] factory model type: roberta-large\n",
      "[Jun 04, 19:54:51] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Jun 04, 19:54:53] get query model type: roberta-large\n",
      "[Jun 04, 19:54:54] get doc model type: roberta-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/irl-tableqa/jaydeep/tu4.24.0/lib/python3.7/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "/dccstor/irl-tableqa/jaydeep/tu4.24.0/lib/python3.7/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 04, 19:54:54] [0] \t\t # of sampled PIDs = 11 \t sampled_pids[:3] = [6, 0, 4]\n",
      "[Jun 04, 19:54:54] [0] \t\t #> Encoding 11 passages..\n",
      "[Jun 04, 19:54:54] #> checkpoint, docFromText, Input: title | text, \t\t 64\n",
      "[Jun 04, 19:54:54] #> Roberta DocTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "[Jun 04, 19:54:54] #> Input: $ title | text, \t\t 64\n",
      "[Jun 04, 19:54:54] #> Output IDs: torch.Size([159]), tensor([    0, 50262,  1270,  1721,  2788,     2,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1])\n",
      "[Jun 04, 19:54:54] #> Output Mask: torch.Size([159]), tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "[Jun 04, 19:54:54] #> checkpoint, docFromText, Output IDs: (tensor([[    0, 50262,  1270,  ...,     1,     1,     1],\n",
      "        [    0, 50262,    22,  ...,     1,     1,     1],\n",
      "        [    0, 50262,    22,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0, 50262,    22,  ...,     1,     1,     1],\n",
      "        [    0, 50262,    22,  ...,     1,     1,     1],\n",
      "        [    0, 50262,    22,  ...,     1,     1,     1]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]))\n",
      "[Jun 04, 19:54:54] #>>>> colbert doc ==\n",
      "[Jun 04, 19:54:54] #>>>>> input_ids: torch.Size([159]), tensor([    0, 50262,  1270,  1721,  2788,     2,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1])\n",
      "[Jun 04, 19:54:59] #>>>> before linear doc ==\n",
      "[Jun 04, 19:54:59] #>>>>> D: torch.Size([159, 1024]), tensor([[-0.6161, -0.7938, -1.0386,  ...,  0.2401,  0.9345,  0.5298],\n",
      "        [ 0.2749, -1.1598, -0.7067,  ...,  0.5915,  1.3987,  1.2713],\n",
      "        [-0.0284, -1.3848, -0.3329,  ...,  0.3906,  1.7883,  0.5797],\n",
      "        ...,\n",
      "        [ 0.2049, -0.9493, -0.4944,  ...,  0.6183,  1.2296,  1.2579],\n",
      "        [ 0.2049, -0.9493, -0.4944,  ...,  0.6183,  1.2296,  1.2579],\n",
      "        [ 0.2049, -0.9493, -0.4944,  ...,  0.6183,  1.2296,  1.2579]])\n",
      "[Jun 04, 19:54:59] #>>>>> self.linear doc : Parameter containing:\n",
      "tensor([[-3.0738e-02,  2.1602e-03,  3.6676e-02,  ..., -2.8078e-03,\n",
      "          2.0939e-02,  1.6086e-02],\n",
      "        [ 1.5858e-02, -1.4224e-02, -1.4469e-05,  ...,  2.9249e-02,\n",
      "         -8.7473e-04, -2.0210e-02],\n",
      "        [ 1.5264e-02,  2.7762e-03, -6.8552e-03,  ...,  8.7342e-03,\n",
      "          6.7920e-03,  3.1651e-03],\n",
      "        ...,\n",
      "        [-2.3147e-03,  3.5463e-02, -3.9315e-03,  ..., -6.7647e-03,\n",
      "          9.7542e-03, -5.3362e-02],\n",
      "        [-2.0770e-02, -2.8881e-02, -1.6047e-02,  ..., -1.5005e-02,\n",
      "          1.6194e-02, -6.9083e-03],\n",
      "        [ 3.1504e-02, -8.3395e-03,  1.1636e-03,  ..., -2.6675e-02,\n",
      "          1.1555e-02,  2.7825e-02]], requires_grad=True)\n",
      "[Jun 04, 19:54:59] #>>>> colbert doc ==\n",
      "[Jun 04, 19:54:59] #>>>>> D: torch.Size([159, 128]), tensor([[-0.7747,  0.6183,  0.3650,  ...,  0.7927, -0.4596,  0.8091],\n",
      "        [-0.5594,  0.7684,  0.6692,  ...,  0.7130, -0.8679,  0.5439],\n",
      "        [-0.4377,  0.4801,  0.6009,  ...,  0.3446, -0.9019,  0.5193],\n",
      "        ...,\n",
      "        [-0.7949,  0.6527,  0.5152,  ...,  0.6576, -0.9798,  0.5175],\n",
      "        [-0.7949,  0.6527,  0.5152,  ...,  0.6576, -0.9798,  0.5175],\n",
      "        [-0.7949,  0.6527,  0.5152,  ...,  0.6576, -0.9798,  0.5175]])\n",
      "[Jun 04, 19:54:59] [0] \t\t avg_doclen_est = 146.81817626953125 \t len(local_sample) = 11\n",
      "[Jun 04, 19:54:59] >> num_partitions_multiplier = 8, self.num_partitions = 256\n",
      "[Jun 04, 19:54:59] [0] \t\t Creaing 256 partitions.\n",
      "[Jun 04, 19:54:59] [0] \t\t *Estimated* 1,614 embeddings.\n",
      "[Jun 04, 19:54:59] [0] \t\t #> Saving the indexing plan to index_root/index_name/plan.json ..\n",
      "Clustering 1535 points in 128D to 256 clusters, redo 1 times, 4 iterations\n",
      "  Preprocessing in 0.00 s\n",
      "  Iteration 3 (0.07 s, search 0.05 s): objective=80.5059 imbalance=3.782 nsplit=0        \n",
      "[0.018, 0.015, 0.016, 0.016, 0.017, 0.017, 0.017, 0.014, 0.015, 0.019, 0.015, 0.016, 0.015, 0.019, 0.018, 0.017, 0.017, 0.018, 0.014, 0.015, 0.015, 0.018, 0.015, 0.022, 0.013, 0.015, 0.013, 0.016, 0.016, 0.015, 0.022, 0.014, 0.018, 0.017, 0.02, 0.017, 0.019, 0.014, 0.02, 0.016, 0.018, 0.017, 0.018, 0.022, 0.016, 0.014, 0.016, 0.015, 0.015, 0.018, 0.016, 0.015, 0.017, 0.015, 0.02, 0.015, 0.017, 0.018, 0.017, 0.014, 0.017, 0.016, 0.014, 0.019, 0.016, 0.016, 0.017, 0.02, 0.015, 0.015, 0.018, 0.015, 0.014, 0.017, 0.017, 0.018, 0.018, 0.018, 0.017, 0.015, 0.012, 0.02, 0.016, 0.019, 0.014, 0.019, 0.019, 0.018, 0.014, 0.019, 0.021, 0.02, 0.016, 0.016, 0.016, 0.015, 0.019, 0.019, 0.016, 0.016, 0.017, 0.015, 0.018, 0.014, 0.015, 0.018, 0.019, 0.013, 0.018, 0.016, 0.017, 0.016, 0.017, 0.017, 0.021, 0.013, 0.016, 0.02, 0.018, 0.017, 0.012, 0.019, 0.015, 0.02, 0.019, 0.018, 0.018, 0.017]\n",
      "[Jun 04, 19:54:59] #> Got bucket_cutoffs_quantiles = tensor([0.5000]) and bucket_weights_quantiles = tensor([0.2500, 0.7500])\n",
      "[Jun 04, 19:54:59] #> Got bucket_cutoffs = tensor([0.]) and bucket_weights = tensor([-0.0111,  0.0111])\n",
      "[Jun 04, 19:54:59] avg_residual = 0.01676233671605587\n",
      "[Jun 04, 19:54:59] #> base_config.py from_path index_root/index_name/metadata.json\n",
      "[Jun 04, 19:54:59] #> base_config.py from_path index_root/index_name/plan.json\n",
      "[Jun 04, 19:54:59] #> base_config.py from_path args loaded! \n",
      "[Jun 04, 19:54:59] #> base_config.py from_path args replaced ! \n",
      "[Jun 04, 19:54:59] [0] \t\t #> Encoding 11 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 1535 points to 256 centroids: please provide at least 9984 training points\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 04, 19:55:04] [0] \t\t #> Saving chunk 0: \t 11 passages and 1,615 embeddings. From #0 onward.\n",
      "[Jun 04, 19:55:04] offset: 0\n",
      "[Jun 04, 19:55:04] chunk codes size(0): 1615\n",
      "[Jun 04, 19:55:04] codes size(0): 1615\n",
      "[Jun 04, 19:55:04] codes size(): torch.Size([1615])\n",
      "[Jun 04, 19:55:04] >>>>partition.size(0): 256\n",
      "[Jun 04, 19:55:04] >>>>num_partition: 256\n",
      "[Jun 04, 19:55:04] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Jun 04, 19:55:04] #> Building the emb2pid mapping..\n",
      "[Jun 04, 19:55:04] len(emb2pid) = 1615\n",
      "[Jun 04, 19:55:04] #> Saved optimized IVF to index_root/index_name/ivf.pid.pt\n",
      "[Jun 04, 19:55:04] [0] \t\t #> Saving the indexing metadata to index_root/index_name/metadata.json ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:04,  4.84s/it]\n",
      "100%|██████████| 256/256 [00:00<00:00, 92452.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#> Joined...\n"
     ]
    }
   ],
   "source": [
    "#change it to pre-processed file location as given in 1st step\n",
    "indexer.index_documents(\"/dccstor/irl-tableqa/jaydeep/sample-document-store2.tsv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20b409ad",
   "metadata": {},
   "source": [
    "## Initializing the Retriever\n",
    "\n",
    "We initialize a ColBERT retriever to search documents from the indexed document corpus.  Note: since we will retrieve the documents based on questions so we need to embed the questions too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eacb6a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.indexer.index_name index_name\n",
      "self.index_root index_root\n",
      "self.index_name index_name\n",
      "Indexer: get collection returned as : /dccstor/irl-tableqa/jaydeep/sample-document-store2.tsv\n",
      "self.collection /dccstor/irl-tableqa/jaydeep/sample-document-store2.tsv\n",
      "self._config ColBERTConfig(ncells=None, centroid_score_threshold=None, ndocs=None, index_path='index_root/index_name', index_location=None, nbits=1, kmeans_niters=20, num_partitions_max=10000000, similarity='cosine', bsize=32, accumsteps=1, lr=3e-06, maxsteps=500000, save_every=None, resume=False, resume_optimizer=False, warmup=None, warmup_bert=None, relu=False, nway=2, use_ib_negatives=False, reranker=False, distillation_alpha=1.0, ignore_scores=False, shuffle_every_epoch=False, save_steps=2000, save_epochs=-1, epochs=10, input_arguments={}, model_type='bert-base-uncased', init_from_lm=None, local_models_repository=None, ranks_fn=None, output_dir=None, topK=100, student_teacher_temperature=1.0, student_teacher_top_loss_weight=0.5, teacher_model_type='xlm-roberta-base', teacher_doc_maxlen=180, distill_query_passage_separately=False, query_only=False, loss_function=None, query_weight=0.5, rng_seed=12345, query_maxlen=32, attend_to_mask_tokens=False, interaction='colbert', dim=128, doc_maxlen=180, mask_punctuation=True, checkpoint=None, teacher_checkpoint=None, triples=None, teacher_triples=None, collection=None, queries=None, index_name='index_name', overwrite=False, root='/dccstor/irl-tableqa/jaydeep/primeqa/docs/tutorials/experiments', experiment='default', index_root='index_root', name='2023-06/04/19.53.49', rank=0, nranks=1, amp=True, gpus=0)\n",
      "[Jun 04, 19:55:27] #> base_config.py from_path index_root/index_name/metadata.json\n",
      "[Jun 04, 19:55:27] #> base_config.py from_path args loaded! \n",
      "[Jun 04, 19:55:27] #> base_config.py from_path args replaced ! \n",
      "[Jun 04, 19:55:29] #>>>>> at ColBERT name (model type) : /dccstor/colbert-ir/bsiyer/PQLL/experiments/xor_squad_04182023/2023-04/22/17.23.31/checkpoints/colbert.dnn.batch_17524.model\n",
      "[Jun 04, 19:55:29] #>>>>> at BaseColBERT name (model type) : /dccstor/colbert-ir/bsiyer/PQLL/experiments/xor_squad_04182023/2023-04/22/17.23.31/checkpoints/colbert.dnn.batch_17524.model\n",
      "[Jun 04, 19:55:32] factory model type: roberta-large\n",
      "[Jun 04, 19:55:41] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Jun 04, 19:55:44] get query model type: roberta-large\n",
      "[Jun 04, 19:55:44] get doc model type: roberta-large\n",
      "[Jun 04, 19:55:45] #> Loading codec...\n",
      "[Jun 04, 19:55:45] #> base_config.py from_path index_root/index_name/metadata.json\n",
      "[Jun 04, 19:55:45] #> base_config.py from_path args loaded! \n",
      "[Jun 04, 19:55:45] #> base_config.py from_path args replaced ! \n",
      "[Jun 04, 19:55:45] #> Loading IVF...\n",
      "[Jun 04, 19:55:45] Loading segmented_lookup_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/irl-tableqa/jaydeep/tu4.24.0/lib/python3.7/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 04, 19:55:46] Loading filter_pids_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Jun 04, 19:55:47] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:00, 20551.16it/s]\n"
     ]
    }
   ],
   "source": [
    "from primeqa.components.retriever.dense import ColBERTRetriever\n",
    "# retriever = ColBERTRetriever(ColBERTIndexerindexer=indexer,\n",
    "#                       query_embedding_model = \"/dccstor/colbert-ir/bsiyer/PQLL/experiments/xor_squad_04182023/2023-04/22/17.23.31/checkpoints/colbert.dnn.batch_17524.model\",\n",
    "#                       use_gpu=True, embed_title=True)\n",
    "retriever = ColBERTRetriever(indexer=indexer,\n",
    "                      checkpoint = \"/dccstor/colbert-ir/bsiyer/PQLL/experiments/xor_squad_04182023/2023-04/22/17.23.31/checkpoints/colbert.dnn.batch_17524.model\"\n",
    "                       )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab15199a",
   "metadata": {},
   "source": [
    "## Start asking Questions\n",
    "\n",
    "We're now ready to query the index we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26e6d79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/irl-tableqa/jaydeep/tu4.24.0/lib/python3.7/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "100%|██████████| 2/2 [00:00<00:00, 44.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[[2, 1, 5], [9, 5, 5]]\n",
      "2\n",
      "[['\"\"\"Albert Einstein\"\"\"\\n \"Albert Einstein Albert Einstein (; ; 14 March 1879 – 18 April 1955) was a German-born theoretical physicist who developed the theory of relativity, one of the two pillars of modern physics (alongside quantum mechanics). His work is also known for its influence on the philosophy of science. He is best known to the general public for his mass–energy equivalence formula , which has been dubbed \"\"the world\\'s most famous equation\"\". He received the 1921 Nobel Prize in Physics \"\"for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect\"\", a pivotal step\"', '\"\"\"Albert Einstein\"\"\"\\n \"to Einstein in 1922. Footnotes Citations Albert Einstein Albert Einstein (; ; 14 March 1879 – 18 April 1955) was a German-born theoretical physicist who developed the theory of relativity, one of the two pillars of modern physics (alongside quantum mechanics). His work is also known for its influence on the philosophy of science. He is best known to the general public for his mass–energy equivalence formula , which has been dubbed \"\"the world\\'s most famous equation\"\". He received the 1921 Nobel Prize in Physics \"\"for his services to theoretical physics, and especially for his discovery of the law of\"', '\"\"\"Alfred Nobel\"\"\"\\n \"was adopted as the standard technology for mining in the \"\"Age of Engineering\"\" bringing Nobel a great amount of financial success, though at a significant cost to his health. An offshoot of this research resulted in Nobel\\'s invention of ballistite, the precursor of many modern smokeless powder explosives and still used as a rocket propellant. In 1888 Alfred\\'s brother Ludvig died while visiting Cannes and a French newspaper erroneously published Alfred\\'s obituary. It condemned him for his invention of dynamite and is said to have brought about his decision to leave a better legacy after his death. The obituary stated,\"'], ['\"\"\"Apple Inc.\"\"\"\\n \"2016, Apple introduced the iPhone 7 and the iPhone 7 Plus, which feature improved system and graphics performance, add water resistance, a new rear dual-camera system on the 7 Plus model, and, controversially, remove the 3.5 mm headphone jack. On September 12, 2017, Apple introduced the iPhone 8 and iPhone 8 Plus, standing as evolutionary updates to its previous phones with a faster processor, improved display technology, upgraded camera systems and wireless charging. The company also announced iPhone X, which radically changes the hardware of the iPhone lineup, removing the home button in favor of facial recognition technology and featuring\"', '\"\"\"Alfred Nobel\"\"\"\\n \"was adopted as the standard technology for mining in the \"\"Age of Engineering\"\" bringing Nobel a great amount of financial success, though at a significant cost to his health. An offshoot of this research resulted in Nobel\\'s invention of ballistite, the precursor of many modern smokeless powder explosives and still used as a rocket propellant. In 1888 Alfred\\'s brother Ludvig died while visiting Cannes and a French newspaper erroneously published Alfred\\'s obituary. It condemned him for his invention of dynamite and is said to have brought about his decision to leave a better legacy after his death. The obituary stated,\"', '\"\"\"Alfred Nobel\"\"\"\\n \"was adopted as the standard technology for mining in the \"\"Age of Engineering\"\" bringing Nobel a great amount of financial success, though at a significant cost to his health. An offshoot of this research resulted in Nobel\\'s invention of ballistite, the precursor of many modern smokeless powder explosives and still used as a rocket propellant. In 1888 Alfred\\'s brother Ludvig died while visiting Cannes and a French newspaper erroneously published Alfred\\'s obituary. It condemned him for his invention of dynamite and is said to have brought about his decision to leave a better legacy after his death. The obituary stated,\"']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 42.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[[(2, 24.019685745239258), (1, 23.459884643554688), (5, 17.92898941040039)], [(9, 14.543024063110352), (5, 4.002993583679199), (5, 4.002993583679199)]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "question = ['What are some famous inventions by Einstein', \"When did Aple introduce iPhone 7\"]\n",
    "retrieved_doc_ids, passages = retriever.predict(input_texts = question, mode = 'query_list',return_passages=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7d3c2d5",
   "metadata": {},
   "source": [
    "Here are the retrived results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed63e82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "print(json.dumps(passages, indent = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
