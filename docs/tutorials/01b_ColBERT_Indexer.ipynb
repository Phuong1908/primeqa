{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "764f905e",
   "metadata": {},
   "source": [
    "# Tutorial: Build a search index using DPR #\n",
    "\n",
    "In this tutorial, we will learn how to build a Neural Search index over your document collection. The algorithm displayed here is called Dense Passage Retrieval (DPR) as described in Karpukhin et al., \"Dense Passage Retrieval for Open-Domain Question Answering\" [here](https://arxiv.org/pdf/2004.04906.pdf).\n",
    "\n",
    "For the purposes of making this tutorial easy to understand we show the steps using a very small document collection. Note that this technique can be used to scale to millions of documents. We have tested upto 21 million Wikipedia passages!!!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4fca36ec",
   "metadata": {},
   "source": [
    "## Preparing a Colab Environment to run this tutorial ##\n",
    "\n",
    "Make sure to \"Enable GPU Runtime\" -> make a URL with a page with screenshots on how to do this."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5e3426d",
   "metadata": {},
   "source": [
    "## Installing PrimeQA\n",
    "\n",
    "First, we need to include the required modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0c4d5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process is terminated.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "pip install --upgrade pip\n",
    "pip install primeqa"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e289b31",
   "metadata": {},
   "source": [
    "## Pre-process your document collection here to be ready to be stored in your Neural Search Index.\n",
    "\n",
    "TODO- add some steps after this to ingest from the sample wikipedia docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c03fd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='/opt/share/cuda-11.1/x86_64'\n",
      "{\"time\":\"2023-06-01 07:33:32,811\", \"name\": \"faiss.loader\", \"level\": \"INFO\", \"message\": \"Loading faiss.\"}\n",
      "{\"time\":\"2023-06-01 07:33:32,831\", \"name\": \"faiss.loader\", \"level\": \"INFO\", \"message\": \"Successfully loaded faiss.\"}\n"
     ]
    }
   ],
   "source": [
    "from primeqa.components.indexer.dense import ColBERTIndexer "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "198cfd78",
   "metadata": {},
   "source": [
    "## Initializing the Indexer\n",
    "\n",
    "We initialize a ColBERT indexer which will be used for indexing the embeddings created for each document (passage) in the collection. It takes a passage_embedding_model to create the embedding vectors and a vector_db specification where it stores the embedding vectors to search later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "346c9aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#indexer= ColBERTIndexer (passage_embedding_model = \"/dccstor/colbert-ir/bsiyer/PQLL/experiments/xor_squad_04182023/2023-04/22/17.23.31/checkpoints/colbert.dnn.batch_17524.model\")\n",
    "#ToDO checkpoint to be renamed to passage_embedding_model\n",
    "indexer= ColBERTIndexer (checkpoint = \"/dccstor/colbert-ir/bsiyer/PQLL/experiments/xor_squad_04182023/2023-04/22/17.23.31/checkpoints/colbert.dnn.batch_17524.model\", vector_db='FAISS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c82a4580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[Jun 01, 07:34:26] #> Creating directory index_root/index_name \n",
      "\n",
      "\n",
      "#> Starting...\n",
      "No CUDA runtime is found, using CUDA_HOME='/opt/share/cuda-11.1/x86_64'\n",
      "{\"time\":\"2023-06-01 07:34:29,172\", \"name\": \"faiss.loader\", \"level\": \"INFO\", \"message\": \"Loading faiss.\"}\n",
      "{\"time\":\"2023-06-01 07:34:29,266\", \"name\": \"faiss.loader\", \"level\": \"INFO\", \"message\": \"Successfully loaded faiss.\"}\n",
      "{\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"index_path\": \"index_root\\/index_name\",\n",
      "    \"index_location\": null,\n",
      "    \"nbits\": 1,\n",
      "    \"kmeans_niters\": 4,\n",
      "    \"num_partitions_max\": 10000000,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 64,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 3e-6,\n",
      "    \"maxsteps\": 500000,\n",
      "    \"save_every\": null,\n",
      "    \"resume\": false,\n",
      "    \"resume_optimizer\": false,\n",
      "    \"warmup\": null,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 2,\n",
      "    \"use_ib_negatives\": false,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"shuffle_every_epoch\": false,\n",
      "    \"save_steps\": 2000,\n",
      "    \"save_epochs\": -1,\n",
      "    \"epochs\": 10,\n",
      "    \"input_arguments\": {},\n",
      "    \"model_type\": \"roberta-large\",\n",
      "    \"init_from_lm\": null,\n",
      "    \"local_models_repository\": null,\n",
      "    \"ranks_fn\": null,\n",
      "    \"output_dir\": null,\n",
      "    \"topK\": 100,\n",
      "    \"student_teacher_temperature\": 1.0,\n",
      "    \"student_teacher_top_loss_weight\": 0.5,\n",
      "    \"teacher_model_type\": \"xlm-roberta-base\",\n",
      "    \"teacher_doc_maxlen\": 180,\n",
      "    \"distill_query_passage_separately\": false,\n",
      "    \"query_only\": false,\n",
      "    \"loss_function\": null,\n",
      "    \"query_weight\": 0.5,\n",
      "    \"rng_seed\": 12345,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 128,\n",
      "    \"doc_maxlen\": 180,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"\\/dccstor\\/colbert-ir\\/bsiyer\\/PQLL\\/experiments\\/xor_squad_04182023\\/2023-04\\/22\\/17.23.31\\/checkpoints\\/colbert.dnn.batch_17524.model\",\n",
      "    \"teacher_checkpoint\": null,\n",
      "    \"triples\": null,\n",
      "    \"teacher_triples\": null,\n",
      "    \"collection\": \"\\/dccstor\\/irl-tableqa\\/jaydeep\\/sample-document-store2.tsv\",\n",
      "    \"queries\": null,\n",
      "    \"index_name\": \"index_name\",\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\\/dccstor\\/irl-tableqa\\/jaydeep\\/primeqa\\/docs\\/tutorials\\/experiments\",\n",
      "    \"experiment\": \"default\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2023-06\\/01\\/07.33.30\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 0\n",
      "}\n",
      "[Jun 01, 07:34:29] #> Loading collection...\n",
      "0M \n",
      "[Jun 01, 07:34:29] #>>>>> at ColBERT name (model type) : /dccstor/colbert-ir/bsiyer/PQLL/experiments/xor_squad_04182023/2023-04/22/17.23.31/checkpoints/colbert.dnn.batch_17524.model\n",
      "[Jun 01, 07:34:29] #>>>>> at BaseColBERT name (model type) : /dccstor/colbert-ir/bsiyer/PQLL/experiments/xor_squad_04182023/2023-04/22/17.23.31/checkpoints/colbert.dnn.batch_17524.model\n",
      "[Jun 01, 07:34:36] factory model type: roberta-large\n",
      "[Jun 01, 07:34:46] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Jun 01, 07:34:49] get query model type: roberta-large\n",
      "[Jun 01, 07:34:49] get doc model type: roberta-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/irl-tableqa/jaydeep/tu4.24.0/lib/python3.7/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "/dccstor/irl-tableqa/jaydeep/tu4.24.0/lib/python3.7/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 01, 07:34:49] [0] \t\t # of sampled PIDs = 11 \t sampled_pids[:3] = [6, 0, 4]\n",
      "[Jun 01, 07:34:49] [0] \t\t #> Encoding 11 passages..\n",
      "[Jun 01, 07:34:49] #> checkpoint, docFromText, Input: title | text, \t\t 64\n",
      "[Jun 01, 07:34:49] #> Roberta DocTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "[Jun 01, 07:34:49] #> Input: $ title | text, \t\t 64\n",
      "[Jun 01, 07:34:49] #> Output IDs: torch.Size([159]), tensor([    0, 50262,  1270,  1721,  2788,     2,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1])\n",
      "[Jun 01, 07:34:49] #> Output Mask: torch.Size([159]), tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "[Jun 01, 07:34:49] #> checkpoint, docFromText, Output IDs: (tensor([[    0, 50262,  1270,  ...,     1,     1,     1],\n",
      "        [    0, 50262,    22,  ...,     1,     1,     1],\n",
      "        [    0, 50262,    22,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0, 50262,    22,  ...,     1,     1,     1],\n",
      "        [    0, 50262,    22,  ...,     1,     1,     1],\n",
      "        [    0, 50262,    22,  ...,     1,     1,     1]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]))\n",
      "[Jun 01, 07:34:49] #>>>> colbert doc ==\n",
      "[Jun 01, 07:34:49] #>>>>> input_ids: torch.Size([159]), tensor([    0, 50262,  1270,  1721,  2788,     2,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1])\n",
      "[Jun 01, 07:34:55] #>>>> before linear doc ==\n",
      "[Jun 01, 07:34:55] #>>>>> D: torch.Size([159, 1024]), tensor([[-0.6161, -0.7938, -1.0386,  ...,  0.2401,  0.9345,  0.5298],\n",
      "        [ 0.2749, -1.1598, -0.7067,  ...,  0.5915,  1.3987,  1.2713],\n",
      "        [-0.0284, -1.3848, -0.3329,  ...,  0.3906,  1.7883,  0.5797],\n",
      "        ...,\n",
      "        [ 0.2049, -0.9493, -0.4944,  ...,  0.6183,  1.2296,  1.2579],\n",
      "        [ 0.2049, -0.9493, -0.4944,  ...,  0.6183,  1.2296,  1.2579],\n",
      "        [ 0.2049, -0.9493, -0.4944,  ...,  0.6183,  1.2296,  1.2579]])\n",
      "[Jun 01, 07:34:55] #>>>>> self.linear doc : Parameter containing:\n",
      "tensor([[-3.0738e-02,  2.1602e-03,  3.6676e-02,  ..., -2.8078e-03,\n",
      "          2.0939e-02,  1.6086e-02],\n",
      "        [ 1.5858e-02, -1.4224e-02, -1.4469e-05,  ...,  2.9249e-02,\n",
      "         -8.7473e-04, -2.0210e-02],\n",
      "        [ 1.5264e-02,  2.7762e-03, -6.8552e-03,  ...,  8.7342e-03,\n",
      "          6.7920e-03,  3.1651e-03],\n",
      "        ...,\n",
      "        [-2.3147e-03,  3.5463e-02, -3.9315e-03,  ..., -6.7647e-03,\n",
      "          9.7542e-03, -5.3362e-02],\n",
      "        [-2.0770e-02, -2.8881e-02, -1.6047e-02,  ..., -1.5005e-02,\n",
      "          1.6194e-02, -6.9083e-03],\n",
      "        [ 3.1504e-02, -8.3395e-03,  1.1636e-03,  ..., -2.6675e-02,\n",
      "          1.1555e-02,  2.7825e-02]], requires_grad=True)\n",
      "[Jun 01, 07:34:55] #>>>> colbert doc ==\n",
      "[Jun 01, 07:34:55] #>>>>> D: torch.Size([159, 128]), tensor([[-0.7747,  0.6183,  0.3650,  ...,  0.7927, -0.4596,  0.8091],\n",
      "        [-0.5594,  0.7684,  0.6692,  ...,  0.7130, -0.8679,  0.5439],\n",
      "        [-0.4377,  0.4801,  0.6009,  ...,  0.3446, -0.9019,  0.5193],\n",
      "        ...,\n",
      "        [-0.7949,  0.6527,  0.5152,  ...,  0.6576, -0.9798,  0.5175],\n",
      "        [-0.7949,  0.6527,  0.5152,  ...,  0.6576, -0.9798,  0.5175],\n",
      "        [-0.7949,  0.6527,  0.5152,  ...,  0.6576, -0.9798,  0.5175]])\n",
      "[Jun 01, 07:34:55] [0] \t\t avg_doclen_est = 146.81817626953125 \t len(local_sample) = 11\n",
      "[Jun 01, 07:34:55] >> num_partitions_multiplier = 8, self.num_partitions = 256\n",
      "[Jun 01, 07:34:55] [0] \t\t Creaing 256 partitions.\n",
      "[Jun 01, 07:34:55] [0] \t\t *Estimated* 1,614 embeddings.\n",
      "[Jun 01, 07:34:55] [0] \t\t #> Saving the indexing plan to index_root/index_name/plan.json ..\n",
      "Clustering 1535 points in 128D to 256 clusters, redo 1 times, 4 iterations\n",
      "  Preprocessing in 0.00 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 1535 points to 256 centroids: please provide at least 9984 training points\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration 3 (0.12 s, search 0.10 s): objective=80.5059 imbalance=3.782 nsplit=0        \n",
      "[0.018, 0.015, 0.016, 0.016, 0.017, 0.017, 0.017, 0.014, 0.015, 0.019, 0.015, 0.016, 0.015, 0.019, 0.018, 0.017, 0.017, 0.018, 0.014, 0.015, 0.015, 0.018, 0.015, 0.022, 0.013, 0.015, 0.013, 0.016, 0.016, 0.015, 0.022, 0.014, 0.018, 0.017, 0.02, 0.017, 0.019, 0.014, 0.02, 0.016, 0.018, 0.017, 0.018, 0.022, 0.016, 0.014, 0.016, 0.015, 0.015, 0.018, 0.016, 0.015, 0.017, 0.015, 0.02, 0.015, 0.017, 0.018, 0.017, 0.014, 0.017, 0.016, 0.014, 0.019, 0.016, 0.016, 0.017, 0.02, 0.015, 0.015, 0.018, 0.015, 0.014, 0.017, 0.017, 0.018, 0.018, 0.018, 0.017, 0.015, 0.012, 0.02, 0.016, 0.019, 0.014, 0.019, 0.019, 0.018, 0.014, 0.019, 0.021, 0.02, 0.016, 0.016, 0.016, 0.015, 0.019, 0.019, 0.016, 0.016, 0.017, 0.015, 0.018, 0.014, 0.015, 0.018, 0.019, 0.013, 0.018, 0.016, 0.017, 0.016, 0.017, 0.017, 0.021, 0.013, 0.016, 0.02, 0.018, 0.017, 0.012, 0.019, 0.015, 0.02, 0.019, 0.018, 0.018, 0.017]\n",
      "[Jun 01, 07:34:55] #> Got bucket_cutoffs_quantiles = tensor([0.5000]) and bucket_weights_quantiles = tensor([0.2500, 0.7500])\n",
      "[Jun 01, 07:34:55] #> Got bucket_cutoffs = tensor([0.]) and bucket_weights = tensor([-0.0111,  0.0111])\n",
      "[Jun 01, 07:34:55] avg_residual = 0.01676233671605587\n",
      "[Jun 01, 07:34:55] #> base_config.py from_path index_root/index_name/metadata.json\n",
      "[Jun 01, 07:34:55] #> base_config.py from_path index_root/index_name/plan.json\n",
      "[Jun 01, 07:34:55] #> base_config.py from_path args loaded! \n",
      "[Jun 01, 07:34:55] #> base_config.py from_path args replaced ! \n",
      "[Jun 01, 07:34:55] [0] \t\t #> Encoding 11 passages..\n",
      "[Jun 01, 07:35:00] [0] \t\t #> Saving chunk 0: \t 11 passages and 1,615 embeddings. From #0 onward.\n",
      "[Jun 01, 07:35:00] offset: 0\n",
      "[Jun 01, 07:35:00] chunk codes size(0): 1615\n",
      "[Jun 01, 07:35:00] codes size(0): 1615\n",
      "[Jun 01, 07:35:00] codes size(): torch.Size([1615])\n",
      "[Jun 01, 07:35:00] >>>>partition.size(0): 256\n",
      "[Jun 01, 07:35:00] >>>>num_partition: 256\n",
      "[Jun 01, 07:35:00] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Jun 01, 07:35:00] #> Building the emb2pid mapping..\n",
      "[Jun 01, 07:35:00] len(emb2pid) = 1615\n",
      "[Jun 01, 07:35:00] #> Saved optimized IVF to index_root/index_name/ivf.pid.pt\n",
      "[Jun 01, 07:35:00] [0] \t\t #> Saving the indexing metadata to index_root/index_name/metadata.json ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:04,  4.96s/it]\n",
      "100%|██████████| 256/256 [00:00<00:00, 88989.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#> Joined...\n"
     ]
    }
   ],
   "source": [
    "#change it to pre-processed file location as given in 1st step\n",
    "indexer.index_documents(\"/dccstor/irl-tableqa/jaydeep/sample-document-store2.tsv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20b409ad",
   "metadata": {},
   "source": [
    "## Initializing the Retriever\n",
    "\n",
    "We initialize a ColBERT retriever to search documents from the indexed document corpus.  Note: since we will retrieve the documents based on questions so we need to embed the questions too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacb6a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from primeqa.components.retriever.dense import ColBERTRetriever\n",
    "# retriever = ColBERTRetriever(ColBERTIndexerindexer=indexer,\n",
    "#                       query_embedding_model = \"/dccstor/colbert-ir/bsiyer/PQLL/experiments/xor_squad_04182023/2023-04/22/17.23.31/checkpoints/colbert.dnn.batch_17524.model\",\n",
    "#                       use_gpu=True, embed_title=True)\n",
    "retriever = ColBERTRetriever(indexer=indexer,\n",
    "                      checkpoint = \"/dccstor/colbert-ir/bsiyer/PQLL/experiments/xor_squad_04182023/2023-04/22/17.23.31/checkpoints/colbert.dnn.batch_17524.model\"\n",
    "                       )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab15199a",
   "metadata": {},
   "source": [
    "## Start asking Questions\n",
    "\n",
    "We're now ready to query the index we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e6d79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = ['Who maintained the throne for the longest time in China?']\n",
    "retrieved_doc_ids, passages = retriever.search(query = question, top_k = 1, mode = 'query_list')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7d3c2d5",
   "metadata": {},
   "source": [
    "Here are the retrived results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed63e82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "print(json.dumps(passages, indent = 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
