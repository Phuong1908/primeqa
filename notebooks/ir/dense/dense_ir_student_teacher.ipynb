{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "787e1675",
   "metadata": {},
   "source": [
    "## DR.DECR (Dense Retrieval with Distillation-Enhanced Cross-Lingual Representation) Student/Teacher Training\n",
    "In this notebook, we show how to train a model using Knowledge Distillation (Student/Teacher) to improve performance on Cross-lingual retrieval, as desribed in `Learning Cross-Lingual IR from an English Retriever`, https://arxiv.org/abs/2112.08185.\n",
    "\n",
    "After training the model, we index the data and run search.  \n",
    "\n",
    "\n",
    "In orded to run (almost) instantaneously, we use trivial sizes of training data and collection to search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbd2536",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "If not already done, please make sure to install PrimeQA with notebooks extras before getting started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21c8eded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want CUDA 11 uncomment and run this (for CUDA 10 or CPU you can ignore this line).\n",
    "#! pip install 'torch~=1.11.0' --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "\n",
    "# Uncomment to install PrimeQA from source (pypi package pending).\n",
    "# The path should be the project root (e.g. '.' below).\n",
    "#! pip install .[notebooks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d64fe65",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "First, we need to include the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fa777e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='/opt/share/cuda-10.1/x86_64'\n",
      "{\"time\":\"2022-10-26 08:44:32,566\", \"name\": \"faiss.loader\", \"level\": \"INFO\", \"message\": \"Loading faiss.\"}\n",
      "{\"time\":\"2022-10-26 08:44:33,466\", \"name\": \"faiss.loader\", \"level\": \"INFO\", \"message\": \"Successfully loaded faiss.\"}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    " \n",
    "from primeqa.ir.dense.colbert_top.colbert.utils.utils import create_directory, print_message\n",
    "from primeqa.ir.dense.colbert_top.colbert.infra import Run, RunConfig\n",
    "from primeqa.ir.dense.colbert_top.colbert.infra.config import ColBERTConfig\n",
    "from primeqa.ir.dense.colbert_top.colbert.training.training import train\n",
    "from primeqa.ir.dense.colbert_top.colbert.indexing.collection_indexer import encode\n",
    "from primeqa.ir.dense.colbert_top.colbert.searcher import Searcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322f5139",
   "metadata": {},
   "source": [
    "## Training\n",
    "We will train a ColBERT model using a TSV file containing [query, positive document, negative document] triples. We use the XOR-TyDi dataset, as described here: https://nlp.cs.washington.edu/xorqa/\n",
    "\n",
    "The path in `test_files_location` below points to the location of files used by the notebook, by default it points to the files used by CI testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23f199e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files_location = '../../../tests/resources/ir_dense'\n",
    "model_type = 'xlm-roberta-base'\n",
    "with tempfile.TemporaryDirectory() as working_dir:\n",
    "    output_dir=os.path.join(working_dir, 'output_dir')\n",
    "text_triples_fn = os.path.join(test_files_location, \"xorqa.train_ir_negs_5_poss_1_001pct_at_0pct.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaac315",
   "metadata": {},
   "source": [
    "Here is an example of a training file record:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3eec7578",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>중국에서 가장 오랜기간 왕위를 유지한 인물은 누구인가?</td>\n",
       "      <td>Kangxi Emperor The Kangxi Emperor's reign of 61 years makes him the longest-reigning emperor in Chinese history (although his grandson, the Qianlong Emperor, had the longest period of \"de facto\" power) and one of the longest-reigning rulers in the world. However, since he ascended the throne at the age of seven, actual power was held for six years by four regents and his grandmother, the Grand Empress Dowager Xiaozhuang.</td>\n",
       "      <td>Chiddy Bang new songs from the duo and in November 2009 debuted the group's first free mixtape entitled \"The Swelly Express\". On 28 April 2011 during the first-ever MTV O Music Awards, Anamege broke the Guinness World Record for Longest Freestyle Rap and Longest Marathon Rapping Record by freestyling for 9 hours, 18 minutes, and 22 seconds, stealing the throne from rapper M-Eighty, who originally broke the record in 2009 by rapping for 9 hours, 15 minutes and 15 seconds. Anamege had also beat Canadian rapper D.O. for Longest Marathon Rapping session, the previous record being for 8 hours and 45 minutes.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "data = pd.read_csv(text_triples_fn, sep='\\t', nrows=1, header=None)\n",
    "display(HTML(data.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf6c9b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = {\n",
    "                'root': output_dir,\n",
    "                'experiment': 'test_training_student',\n",
    "                'triples': text_triples_fn,\n",
    "                'model_type': model_type,\n",
    "                'maxsteps': 3,\n",
    "                'bsize': 1,\n",
    "                'accumsteps': 1,\n",
    "                'amp': True,\n",
    "                'epochs': 1,\n",
    "                'rank': 0,\n",
    "                'nranks': 1\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f9433a",
   "metadata": {},
   "source": [
    "Next we train the the student starting-point model, and save it's location in the `student_model_fn`variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eba19fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"index_path\": null,\n",
      "    \"index_location\": null,\n",
      "    \"nbits\": 1,\n",
      "    \"kmeans_niters\": 20,\n",
      "    \"num_partitions_max\": 10000000,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 1,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 3e-6,\n",
      "    \"maxsteps\": 3,\n",
      "    \"save_every\": null,\n",
      "    \"resume\": false,\n",
      "    \"resume_optimizer\": false,\n",
      "    \"warmup\": null,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 2,\n",
      "    \"use_ib_negatives\": false,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"shuffle_every_epoch\": false,\n",
      "    \"save_steps\": 2000,\n",
      "    \"save_epochs\": -1,\n",
      "    \"epochs\": 1,\n",
      "    \"input_arguments\": {},\n",
      "    \"model_type\": \"xlm-roberta-base\",\n",
      "    \"init_from_lm\": null,\n",
      "    \"local_models_repository\": null,\n",
      "    \"ranks_fn\": null,\n",
      "    \"output_dir\": null,\n",
      "    \"topK\": 100,\n",
      "    \"student_teacher_temperature\": 1.0,\n",
      "    \"student_teacher_top_loss_weight\": 0.5,\n",
      "    \"teacher_model_type\": \"xlm-roberta-base\",\n",
      "    \"teacher_doc_maxlen\": 180,\n",
      "    \"distill_query_passage_separately\": false,\n",
      "    \"query_only\": false,\n",
      "    \"loss_function\": null,\n",
      "    \"query_weight\": 0.5,\n",
      "    \"rng_seed\": 12345,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 128,\n",
      "    \"doc_maxlen\": 180,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": null,\n",
      "    \"teacher_checkpoint\": null,\n",
      "    \"triples\": \"..\\/..\\/..\\/tests\\/resources\\/ir_dense\\/xorqa.train_ir_negs_5_poss_1_001pct_at_0pct.tsv\",\n",
      "    \"teacher_triples\": null,\n",
      "    \"collection\": null,\n",
      "    \"queries\": null,\n",
      "    \"index_name\": null,\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\\/tmp\\/tmptrqmqrf1\\/output_dir\",\n",
      "    \"experiment\": \"test_training_student\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2022-10\\/26\\/08.43.55\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 0\n",
      "}\n",
      "[Oct 26, 08:44:54] model type: xlm-roberta-base\n",
      "[Oct 26, 08:44:54] Using config.bsize = 1 (per process) and config.accumsteps = 1\n",
      "[Oct 26, 08:44:54] get query model type: xlm-roberta-base\n",
      "[Oct 26, 08:44:57] get doc model type: xlm-roberta-base\n",
      "[Oct 26, 08:44:58] #> Loading triples...\n",
      "[Oct 26, 08:44:58] #>>>>> at ColBERT name (model type) : xlm-roberta-base\n",
      "[Oct 26, 08:44:59] #>>>>> at BaseColBERT name (model type) : xlm-roberta-base\n",
      "[Oct 26, 08:44:59] #> base_config.py load_from_checkpoint xlm-roberta-base\n",
      "[Oct 26, 08:44:59] #> base_config.py load_from_checkpoint xlm-roberta-base/artifact.metadata\n",
      "[Oct 26, 08:44:59] factory model type: xlm-roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing HF_ColBERT_XLMR: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing HF_ColBERT_XLMR from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing HF_ColBERT_XLMR from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of HF_ColBERT_XLMR were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.pooler.dense.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.pooler.dense.bias', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.output.dense.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'linear.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.5.output.LayerNorm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 26, 08:45:17] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/franzm/packages/minconda3/envs/primeqa_pyarrow/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 26, 08:45:20] maxsteps: 3\n",
      "[Oct 26, 08:45:20] 1 epochs of 5 examples\n",
      "[Oct 26, 08:45:20] batch size: 1\n",
      "[Oct 26, 08:45:20] maxsteps set to 3\n",
      "[Oct 26, 08:45:20] start batch idx: 0\n",
      "[Oct 26, 08:45:21] #> XMLR QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "[Oct 26, 08:45:21] #> Input: $ 중국에서 가장 오랜기간 왕위를 유지한 인물은 누구인가?, \t\t True, \t\t None\n",
      "[Oct 26, 08:45:21] #> Output IDs: torch.Size([32]), tensor([     0,   9748,  24120,   1180,  13968, 211059,  83639,  76826,  78363,\n",
      "         57104,    993, 161732,    697, 116932, 114150,     32,      2,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1])\n",
      "[Oct 26, 08:45:21] #> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "[Oct 26, 08:45:21] #> XLMR DocTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "[Oct 26, 08:45:21] #> Input: $ \"Kangxi Emperor The Kangxi Emperor's reign of 61 years makes him the longest-reigning emperor in Chinese history (although his grandson, the Qianlong Emperor, had the longest period of \"\"de facto\"\" power) and one of the longest-reigning rulers in the world. However, since he ascended the throne at the age of seven, actual power was held for six years by four regents and his grandmother, the Grand Empress Dowager Xiaozhuang.\", \t\t None\n",
      "[Oct 26, 08:45:21] #> Output IDs: torch.Size([155]), tensor([     0,   9749,     44,  50245, 122809,  31678,     56,    748,    581,\n",
      "         30267,   5134,  31678,     56,    748,     25,      7,   1690,  38529,\n",
      "           111,  11716,   5369,  30482,   4049,     70,   4989,    525,      9,\n",
      "           107,    872,    592,      6,  88940,    748,     23,  76438,  32692,\n",
      "            15,    289, 197271,   1919,  69190,    191,      4,     70,  43954,\n",
      "            66,  10617,  31678,     56,    748,      4,   1902,     70,   4989,\n",
      "           525,  14922,    111,     44,     58,    112,  72563,     58,     58,\n",
      "         14537,     16,    136,   1632,    111,     70,   4989,    525,      9,\n",
      "           107,    872,    592,  79986,   4295,     23,     70,   8999,      5,\n",
      "         33306,      4,  16792,    764,  82800,    297,     70,      6,  42294,\n",
      "            86,     99,     70,  32070,    111,  59671,      4,   8561,  14537,\n",
      "           509,  34658,    100,  37195,   5369,    390,  22759, 119555,    933,\n",
      "           136,   1919,   9963,    432,   9319,      4,     70,  12801,   2775,\n",
      "         11856, 145058,  33730, 211423,    169,   1132,   1463,   1242,      2,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1])\n",
      "[Oct 26, 08:45:21] #> Output Mask: torch.Size([155]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "[Oct 26, 08:45:21] #>>>> colbert query ==\n",
      "[Oct 26, 08:45:21] #>>>>> input_ids: torch.Size([32]), tensor([     0,   9748,  24120,   1180,  13968, 211059,  83639,  76826,  78363,\n",
      "         57104,    993, 161732,    697, 116932, 114150,     32,      2,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/franzm/packages/minconda3/envs/primeqa_pyarrow/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "/u/franzm/packages/minconda3/envs/primeqa_pyarrow/lib/python3.9/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 26, 08:45:25] #>>>> before linear query ==\n",
      "[Oct 26, 08:45:25] #>>>>> Q: torch.Size([32, 768]), tensor([[ 0.1269,  0.0868,  0.0588,  ..., -0.0978,  0.0423,  0.0024],\n",
      "        [ 0.0237,  0.0695,  0.0303,  ..., -0.0671, -0.0818, -0.0653],\n",
      "        [ 0.0626,  0.0780,  0.0324,  ...,  0.0213, -0.0027,  0.1598],\n",
      "        ...,\n",
      "        [ 0.1314,  0.0969,  0.0671,  ..., -0.1059,  0.0561, -0.0338],\n",
      "        [ 0.0398,  0.0168,  0.0476,  ..., -0.0210, -0.0167, -0.0480],\n",
      "        [ 0.0871,  0.0832,  0.0208,  ..., -0.0546,  0.0003,  0.0225]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "[Oct 26, 08:45:25] #>>>>> self.linear query : Parameter containing:\n",
      "tensor([[-1.2613e-02, -3.6824e-03, -1.0773e-02,  ..., -1.3896e-02,\n",
      "          1.2244e-02, -8.2326e-03],\n",
      "        [-3.4988e-03,  3.9623e-03, -2.4905e-02,  ..., -2.6214e-02,\n",
      "          4.0837e-03, -8.0812e-03],\n",
      "        [-1.1158e-02, -2.1660e-05,  1.5019e-02,  ..., -1.1310e-02,\n",
      "         -9.3691e-03, -2.2202e-02],\n",
      "        ...,\n",
      "        [-1.0295e-02,  3.2577e-02,  3.3341e-03,  ...,  3.1876e-02,\n",
      "          3.6851e-03,  2.2995e-02],\n",
      "        [ 1.5563e-03,  2.2341e-03,  1.3362e-02,  ..., -2.8300e-02,\n",
      "          3.9382e-03,  1.1980e-02],\n",
      "        [-5.5801e-03,  3.1922e-02, -2.3421e-02,  ..., -1.1762e-04,\n",
      "          1.1104e-02, -9.7669e-03]], requires_grad=True)\n",
      "[Oct 26, 08:45:25] #>>>> colbert query ==\n",
      "[Oct 26, 08:45:25] #>>>>> Q: torch.Size([32, 128]), tensor([[-2.0807e-01, -2.8308e-02, -3.4714e-01,  ..., -1.5437e-01,\n",
      "         -1.1283e-01, -1.9922e-01],\n",
      "        [-5.8863e-02,  2.5084e-01, -4.4934e-01,  ..., -2.2064e-01,\n",
      "         -2.4872e-01, -6.0744e-02],\n",
      "        [ 1.5445e-03,  2.7442e-01, -4.6942e-01,  ..., -3.4489e-01,\n",
      "         -2.1474e-01, -1.7858e-02],\n",
      "        ...,\n",
      "        [-2.0095e-01, -3.4476e-02, -3.4331e-01,  ..., -1.5825e-01,\n",
      "         -1.2848e-01, -1.9769e-01],\n",
      "        [-1.4014e-01,  1.9043e-01, -4.8735e-01,  ..., -2.1481e-01,\n",
      "         -3.0219e-01, -1.2825e-01],\n",
      "        [-1.7410e-01, -2.4073e-04, -3.4931e-01,  ..., -1.7014e-01,\n",
      "         -1.2034e-01, -1.9180e-01]], grad_fn=<SelectBackward0>)\n",
      "[Oct 26, 08:45:25] #>>>> colbert doc ==\n",
      "[Oct 26, 08:45:25] #>>>>> input_ids: torch.Size([155]), tensor([     0,   9749,     44,  50245, 122809,  31678,     56,    748,    581,\n",
      "         30267,   5134,  31678,     56,    748,     25,      7,   1690,  38529,\n",
      "           111,  11716,   5369,  30482,   4049,     70,   4989,    525,      9,\n",
      "           107,    872,    592,      6,  88940,    748,     23,  76438,  32692,\n",
      "            15,    289, 197271,   1919,  69190,    191,      4,     70,  43954,\n",
      "            66,  10617,  31678,     56,    748,      4,   1902,     70,   4989,\n",
      "           525,  14922,    111,     44,     58,    112,  72563,     58,     58,\n",
      "         14537,     16,    136,   1632,    111,     70,   4989,    525,      9,\n",
      "           107,    872,    592,  79986,   4295,     23,     70,   8999,      5,\n",
      "         33306,      4,  16792,    764,  82800,    297,     70,      6,  42294,\n",
      "            86,     99,     70,  32070,    111,  59671,      4,   8561,  14537,\n",
      "           509,  34658,    100,  37195,   5369,    390,  22759, 119555,    933,\n",
      "           136,   1919,   9963,    432,   9319,      4,     70,  12801,   2775,\n",
      "         11856, 145058,  33730, 211423,    169,   1132,   1463,   1242,      2,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1])\n",
      "[Oct 26, 08:45:33] #>>>> before linear doc ==\n",
      "[Oct 26, 08:45:33] #>>>>> D: torch.Size([155, 768]), tensor([[ 0.0867,  0.1007,  0.0726,  ..., -0.0399,  0.0469, -0.0280],\n",
      "        [-0.0560,  0.0295, -0.0113,  ..., -0.0767, -0.0689, -0.0421],\n",
      "        [-0.0180,  0.0234, -0.0086,  ...,  0.0178,  0.0338,  0.0239],\n",
      "        ...,\n",
      "        [ 0.0670,  0.1110,  0.0430,  ..., -0.0891, -0.0012,  0.0213],\n",
      "        [ 0.0930,  0.1011,  0.0189,  ..., -0.1237,  0.0069, -0.0042],\n",
      "        [ 0.0750,  0.1035,  0.0231,  ..., -0.1795, -0.0286,  0.0083]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "[Oct 26, 08:45:33] #>>>>> self.linear doc : Parameter containing:\n",
      "tensor([[-1.2613e-02, -3.6824e-03, -1.0773e-02,  ..., -1.3896e-02,\n",
      "          1.2244e-02, -8.2326e-03],\n",
      "        [-3.4988e-03,  3.9623e-03, -2.4905e-02,  ..., -2.6214e-02,\n",
      "          4.0837e-03, -8.0812e-03],\n",
      "        [-1.1158e-02, -2.1660e-05,  1.5019e-02,  ..., -1.1310e-02,\n",
      "         -9.3691e-03, -2.2202e-02],\n",
      "        ...,\n",
      "        [-1.0295e-02,  3.2577e-02,  3.3341e-03,  ...,  3.1876e-02,\n",
      "          3.6851e-03,  2.2995e-02],\n",
      "        [ 1.5563e-03,  2.2341e-03,  1.3362e-02,  ..., -2.8300e-02,\n",
      "          3.9382e-03,  1.1980e-02],\n",
      "        [-5.5801e-03,  3.1922e-02, -2.3421e-02,  ..., -1.1762e-04,\n",
      "          1.1104e-02, -9.7669e-03]], requires_grad=True)\n",
      "[Oct 26, 08:45:33] #>>>> colbert doc ==\n",
      "[Oct 26, 08:45:33] #>>>>> D: torch.Size([155, 128]), tensor([[-0.1933, -0.0139, -0.3628,  ..., -0.1452, -0.1189, -0.1785],\n",
      "        [-0.0756,  0.2043, -0.4927,  ..., -0.1633, -0.2681, -0.0607],\n",
      "        [-0.0516,  0.2256, -0.5057,  ..., -0.2203, -0.2230, -0.0105],\n",
      "        ...,\n",
      "        [-0.1702,  0.0041, -0.3880,  ..., -0.1042, -0.1178, -0.1780],\n",
      "        [-0.1983, -0.0173, -0.3721,  ..., -0.1311, -0.0983, -0.1707],\n",
      "        [-0.1884,  0.0173, -0.3752,  ..., -0.1619, -0.1253, -0.1812]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "#>>>    30.06 29.8 \t\t|\t\t 0.259999999999998\n",
      "[Oct 26, 08:46:09] 0 0.5736338496208191\n",
      "#>>>    30.38 29.59 \t\t|\t\t 0.7899999999999991\n",
      "[Oct 26, 08:46:58] 1 0.5734368096590042\n",
      "#>>>    30.65 29.46 \t\t|\t\t 1.1899999999999977\n",
      "[Oct 26, 08:47:51] 2 0.5731299413610697\n",
      "#> Saving a checkpoint to /tmp/tmptrqmqrf1/output_dir/test_training_student/2022-10/26/08.43.55/checkpoints/colbert.dnn.batch_3.model ..\n",
      "#> Saving a checkpoint to /tmp/tmptrqmqrf1/output_dir/test_training_student/2022-10/26/08.43.55/checkpoints/colbert-batch_3 ..\n",
      "[Oct 26, 08:48:00] name:/tmp/tmptrqmqrf1/output_dir/test_training_student/2022-10/26/08.43.55/checkpoints/colbert-LAST.dnn\n",
      "[Oct 26, 08:48:00] Make a sym link of /tmp/tmptrqmqrf1/output_dir/test_training_student/2022-10/26/08.43.55/checkpoints/colbert.dnn.batch_3.model to /tmp/tmptrqmqrf1/output_dir/test_training_student/2022-10/26/08.43.55/checkpoints/colbert-LAST.dnn\n",
      "[Oct 26, 08:48:00] #> Done with all triples!\n",
      "#> Saving a checkpoint to /tmp/tmptrqmqrf1/output_dir/test_training_student/2022-10/26/08.43.55/checkpoints/colbert ..\n"
     ]
    }
   ],
   "source": [
    "with Run().context(RunConfig(root=args_dict['root'], experiment=args_dict['experiment'], nranks=args_dict['nranks'], amp=args_dict['amp'])):\n",
    "    colBERTConfig = ColBERTConfig(**args_dict)   \n",
    "    student_model_fn = train(colBERTConfig, text_triples_fn, None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a36c9f",
   "metadata": {},
   "source": [
    "Next we train the the teacher model, and save its location in the `teacher_model_fn`variable.\n",
    "\n",
    "When training this model we use data with the same passages as for the student model, but with English translations of the queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ef157fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_triples_en_fn = os.path.join(test_files_location, \"xorqa.train_ir_negs_5_poss_1_001pct_at_0pct_en.tsv\")\n",
    "args_dict = {\n",
    "                'root': output_dir,\n",
    "                'experiment': 'test_training_teacher',\n",
    "                'triples': text_triples_en_fn,\n",
    "                'model_type': model_type,\n",
    "                'maxsteps': 3,\n",
    "                'bsize': 1,\n",
    "                'accumsteps': 1,\n",
    "                'amp': True,\n",
    "                'epochs': 1,\n",
    "                'rank': 0,\n",
    "                'nranks': 1\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62018427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"index_path\": null,\n",
      "    \"index_location\": null,\n",
      "    \"nbits\": 1,\n",
      "    \"kmeans_niters\": 20,\n",
      "    \"num_partitions_max\": 10000000,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 1,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 3e-6,\n",
      "    \"maxsteps\": 3,\n",
      "    \"save_every\": null,\n",
      "    \"resume\": false,\n",
      "    \"resume_optimizer\": false,\n",
      "    \"warmup\": null,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 2,\n",
      "    \"use_ib_negatives\": false,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"shuffle_every_epoch\": false,\n",
      "    \"save_steps\": 2000,\n",
      "    \"save_epochs\": -1,\n",
      "    \"epochs\": 1,\n",
      "    \"input_arguments\": {},\n",
      "    \"model_type\": \"xlm-roberta-base\",\n",
      "    \"init_from_lm\": null,\n",
      "    \"local_models_repository\": null,\n",
      "    \"ranks_fn\": null,\n",
      "    \"output_dir\": null,\n",
      "    \"topK\": 100,\n",
      "    \"student_teacher_temperature\": 1.0,\n",
      "    \"student_teacher_top_loss_weight\": 0.5,\n",
      "    \"teacher_model_type\": \"xlm-roberta-base\",\n",
      "    \"teacher_doc_maxlen\": 180,\n",
      "    \"distill_query_passage_separately\": false,\n",
      "    \"query_only\": false,\n",
      "    \"loss_function\": null,\n",
      "    \"query_weight\": 0.5,\n",
      "    \"rng_seed\": 12345,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 128,\n",
      "    \"doc_maxlen\": 180,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": null,\n",
      "    \"teacher_checkpoint\": null,\n",
      "    \"triples\": \"..\\/..\\/..\\/tests\\/resources\\/ir_dense\\/xorqa.train_ir_negs_5_poss_1_001pct_at_0pct_en.tsv\",\n",
      "    \"teacher_triples\": null,\n",
      "    \"collection\": null,\n",
      "    \"queries\": null,\n",
      "    \"index_name\": null,\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\\/tmp\\/tmptrqmqrf1\\/output_dir\",\n",
      "    \"experiment\": \"test_training_teacher\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2022-10\\/26\\/08.43.55\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 0\n",
      "}\n",
      "[Oct 26, 08:48:06] model type: xlm-roberta-base\n",
      "[Oct 26, 08:48:06] Using config.bsize = 1 (per process) and config.accumsteps = 1\n",
      "[Oct 26, 08:48:06] get query model type: xlm-roberta-base\n",
      "[Oct 26, 08:48:08] get doc model type: xlm-roberta-base\n",
      "[Oct 26, 08:48:10] #> Loading triples...\n",
      "[Oct 26, 08:48:10] #>>>>> at ColBERT name (model type) : xlm-roberta-base\n",
      "[Oct 26, 08:48:10] #>>>>> at BaseColBERT name (model type) : xlm-roberta-base\n",
      "[Oct 26, 08:48:10] #> base_config.py load_from_checkpoint xlm-roberta-base\n",
      "[Oct 26, 08:48:10] #> base_config.py load_from_checkpoint xlm-roberta-base/artifact.metadata\n",
      "[Oct 26, 08:48:10] factory model type: xlm-roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing HF_ColBERT_XLMR: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing HF_ColBERT_XLMR from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing HF_ColBERT_XLMR from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of HF_ColBERT_XLMR were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.pooler.dense.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.pooler.dense.bias', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.output.dense.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'linear.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.5.output.LayerNorm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 26, 08:48:26] maxsteps: 3\n",
      "[Oct 26, 08:48:26] 1 epochs of 5 examples\n",
      "[Oct 26, 08:48:26] batch size: 1\n",
      "[Oct 26, 08:48:26] maxsteps set to 3\n",
      "[Oct 26, 08:48:26] start batch idx: 0\n",
      "[Oct 26, 08:48:26] #> XMLR QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "[Oct 26, 08:48:26] #> Input: $ Who maintained the throne for the longest time in China?, \t\t True, \t\t None\n",
      "[Oct 26, 08:48:26] #> Output IDs: torch.Size([32]), tensor([    0,  9748, 40469, 76104,   297,    70,     6, 42294,    86,   100,\n",
      "           70,  4989,   525,  1733,    23,  9098,    32,     2,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1])\n",
      "[Oct 26, 08:48:26] #> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "[Oct 26, 08:48:26] #> XLMR DocTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "[Oct 26, 08:48:26] #> Input: $ \"Kangxi Emperor The Kangxi Emperor's reign of 61 years makes him the longest-reigning emperor in Chinese history (although his grandson, the Qianlong Emperor, had the longest period of \"\"de facto\"\" power) and one of the longest-reigning rulers in the world. However, since he ascended the throne at the age of seven, actual power was held for six years by four regents and his grandmother, the Grand Empress Dowager Xiaozhuang.\", \t\t None\n",
      "[Oct 26, 08:48:26] #> Output IDs: torch.Size([155]), tensor([     0,   9749,     44,  50245, 122809,  31678,     56,    748,    581,\n",
      "         30267,   5134,  31678,     56,    748,     25,      7,   1690,  38529,\n",
      "           111,  11716,   5369,  30482,   4049,     70,   4989,    525,      9,\n",
      "           107,    872,    592,      6,  88940,    748,     23,  76438,  32692,\n",
      "            15,    289, 197271,   1919,  69190,    191,      4,     70,  43954,\n",
      "            66,  10617,  31678,     56,    748,      4,   1902,     70,   4989,\n",
      "           525,  14922,    111,     44,     58,    112,  72563,     58,     58,\n",
      "         14537,     16,    136,   1632,    111,     70,   4989,    525,      9,\n",
      "           107,    872,    592,  79986,   4295,     23,     70,   8999,      5,\n",
      "         33306,      4,  16792,    764,  82800,    297,     70,      6,  42294,\n",
      "            86,     99,     70,  32070,    111,  59671,      4,   8561,  14537,\n",
      "           509,  34658,    100,  37195,   5369,    390,  22759, 119555,    933,\n",
      "           136,   1919,   9963,    432,   9319,      4,     70,  12801,   2775,\n",
      "         11856, 145058,  33730, 211423,    169,   1132,   1463,   1242,      2,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1])\n",
      "[Oct 26, 08:48:26] #> Output Mask: torch.Size([155]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "[Oct 26, 08:48:26] #>>>> colbert query ==\n",
      "[Oct 26, 08:48:26] #>>>>> input_ids: torch.Size([32]), tensor([    0,  9748, 40469, 76104,   297,    70,     6, 42294,    86,   100,\n",
      "           70,  4989,   525,  1733,    23,  9098,    32,     2,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/franzm/packages/minconda3/envs/primeqa_pyarrow/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/u/franzm/packages/minconda3/envs/primeqa_pyarrow/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "/u/franzm/packages/minconda3/envs/primeqa_pyarrow/lib/python3.9/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 26, 08:48:31] #>>>> before linear query ==\n",
      "[Oct 26, 08:48:31] #>>>>> Q: torch.Size([32, 768]), tensor([[ 0.1266,  0.1302,  0.0628,  ..., -0.1106,  0.0519, -0.0265],\n",
      "        [-0.0445,  0.0772,  0.0160,  ..., -0.0904, -0.0560, -0.0523],\n",
      "        [-0.0128,  0.0967, -0.0064,  ..., -0.0320, -0.0303,  0.1300],\n",
      "        ...,\n",
      "        [ 0.1200,  0.1188, -0.0589,  ..., -0.2018, -0.0487, -0.0523],\n",
      "        [ 0.0734,  0.0938, -0.0106,  ..., -0.1310, -0.0079,  0.0202],\n",
      "        [ 0.1056,  0.1080, -0.0078,  ..., -0.0156, -0.0259,  0.0222]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "[Oct 26, 08:48:31] #>>>>> self.linear query : Parameter containing:\n",
      "tensor([[-1.2613e-02, -3.6824e-03, -1.0773e-02,  ..., -1.3896e-02,\n",
      "          1.2244e-02, -8.2326e-03],\n",
      "        [-3.4988e-03,  3.9623e-03, -2.4905e-02,  ..., -2.6214e-02,\n",
      "          4.0837e-03, -8.0812e-03],\n",
      "        [-1.1158e-02, -2.1660e-05,  1.5019e-02,  ..., -1.1310e-02,\n",
      "         -9.3691e-03, -2.2202e-02],\n",
      "        ...,\n",
      "        [-1.0295e-02,  3.2577e-02,  3.3341e-03,  ...,  3.1876e-02,\n",
      "          3.6851e-03,  2.2995e-02],\n",
      "        [ 1.5563e-03,  2.2341e-03,  1.3362e-02,  ..., -2.8300e-02,\n",
      "          3.9382e-03,  1.1980e-02],\n",
      "        [-5.5801e-03,  3.1922e-02, -2.3421e-02,  ..., -1.1762e-04,\n",
      "          1.1104e-02, -9.7669e-03]], requires_grad=True)\n",
      "[Oct 26, 08:48:31] #>>>> colbert query ==\n",
      "[Oct 26, 08:48:31] #>>>>> Q: torch.Size([32, 128]), tensor([[-0.2160, -0.0643, -0.3410,  ..., -0.1110, -0.1016, -0.2125],\n",
      "        [-0.1022,  0.1866, -0.4313,  ..., -0.1761, -0.2315, -0.0608],\n",
      "        [-0.0878,  0.2387, -0.4610,  ..., -0.1814, -0.2053, -0.1490],\n",
      "        ...,\n",
      "        [-0.1637, -0.0385, -0.3191,  ..., -0.1197, -0.0936, -0.2225],\n",
      "        [-0.1733, -0.0089, -0.3632,  ..., -0.1557, -0.1166, -0.1823],\n",
      "        [-0.1628, -0.0254, -0.3328,  ..., -0.1277, -0.0925, -0.2184]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "[Oct 26, 08:48:31] #>>>> colbert doc ==\n",
      "[Oct 26, 08:48:31] #>>>>> input_ids: torch.Size([155]), tensor([     0,   9749,     44,  50245, 122809,  31678,     56,    748,    581,\n",
      "         30267,   5134,  31678,     56,    748,     25,      7,   1690,  38529,\n",
      "           111,  11716,   5369,  30482,   4049,     70,   4989,    525,      9,\n",
      "           107,    872,    592,      6,  88940,    748,     23,  76438,  32692,\n",
      "            15,    289, 197271,   1919,  69190,    191,      4,     70,  43954,\n",
      "            66,  10617,  31678,     56,    748,      4,   1902,     70,   4989,\n",
      "           525,  14922,    111,     44,     58,    112,  72563,     58,     58,\n",
      "         14537,     16,    136,   1632,    111,     70,   4989,    525,      9,\n",
      "           107,    872,    592,  79986,   4295,     23,     70,   8999,      5,\n",
      "         33306,      4,  16792,    764,  82800,    297,     70,      6,  42294,\n",
      "            86,     99,     70,  32070,    111,  59671,      4,   8561,  14537,\n",
      "           509,  34658,    100,  37195,   5369,    390,  22759, 119555,    933,\n",
      "           136,   1919,   9963,    432,   9319,      4,     70,  12801,   2775,\n",
      "         11856, 145058,  33730, 211423,    169,   1132,   1463,   1242,      2,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1])\n",
      "[Oct 26, 08:48:41] #>>>> before linear doc ==\n",
      "[Oct 26, 08:48:41] #>>>>> D: torch.Size([155, 768]), tensor([[ 0.0867,  0.1007,  0.0726,  ..., -0.0399,  0.0469, -0.0280],\n",
      "        [-0.0560,  0.0295, -0.0113,  ..., -0.0767, -0.0689, -0.0421],\n",
      "        [-0.0180,  0.0234, -0.0086,  ...,  0.0178,  0.0338,  0.0239],\n",
      "        ...,\n",
      "        [ 0.0670,  0.1110,  0.0430,  ..., -0.0891, -0.0012,  0.0213],\n",
      "        [ 0.0930,  0.1011,  0.0189,  ..., -0.1237,  0.0069, -0.0042],\n",
      "        [ 0.0750,  0.1035,  0.0231,  ..., -0.1795, -0.0286,  0.0083]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "[Oct 26, 08:48:41] #>>>>> self.linear doc : Parameter containing:\n",
      "tensor([[-1.2613e-02, -3.6824e-03, -1.0773e-02,  ..., -1.3896e-02,\n",
      "          1.2244e-02, -8.2326e-03],\n",
      "        [-3.4988e-03,  3.9623e-03, -2.4905e-02,  ..., -2.6214e-02,\n",
      "          4.0837e-03, -8.0812e-03],\n",
      "        [-1.1158e-02, -2.1660e-05,  1.5019e-02,  ..., -1.1310e-02,\n",
      "         -9.3691e-03, -2.2202e-02],\n",
      "        ...,\n",
      "        [-1.0295e-02,  3.2577e-02,  3.3341e-03,  ...,  3.1876e-02,\n",
      "          3.6851e-03,  2.2995e-02],\n",
      "        [ 1.5563e-03,  2.2341e-03,  1.3362e-02,  ..., -2.8300e-02,\n",
      "          3.9382e-03,  1.1980e-02],\n",
      "        [-5.5801e-03,  3.1922e-02, -2.3421e-02,  ..., -1.1762e-04,\n",
      "          1.1104e-02, -9.7669e-03]], requires_grad=True)\n",
      "[Oct 26, 08:48:41] #>>>> colbert doc ==\n",
      "[Oct 26, 08:48:41] #>>>>> D: torch.Size([155, 128]), tensor([[-0.1933, -0.0139, -0.3628,  ..., -0.1452, -0.1189, -0.1785],\n",
      "        [-0.0756,  0.2043, -0.4927,  ..., -0.1633, -0.2681, -0.0607],\n",
      "        [-0.0516,  0.2256, -0.5057,  ..., -0.2203, -0.2230, -0.0105],\n",
      "        ...,\n",
      "        [-0.1702,  0.0041, -0.3880,  ..., -0.1042, -0.1178, -0.1780],\n",
      "        [-0.1983, -0.0173, -0.3721,  ..., -0.1311, -0.0983, -0.1707],\n",
      "        [-0.1884,  0.0173, -0.3752,  ..., -0.1619, -0.1253, -0.1812]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "#>>>    30.01 29.85 \t\t|\t\t 0.16000000000000014\n",
      "[Oct 26, 08:49:21] 0 0.6181421279907227\n",
      "#>>>    30.68 29.72 \t\t|\t\t 0.9600000000000009\n",
      "[Oct 26, 08:50:16] 1 0.6178465316593647\n",
      "#>>>    30.71 29.49 \t\t|\t\t 1.2200000000000024\n",
      "[Oct 26, 08:51:13] 2 0.6174877351691423\n",
      "#> Saving a checkpoint to /tmp/tmptrqmqrf1/output_dir/test_training_teacher/2022-10/26/08.43.55/checkpoints/colbert.dnn.batch_3.model ..\n",
      "#> Saving a checkpoint to /tmp/tmptrqmqrf1/output_dir/test_training_teacher/2022-10/26/08.43.55/checkpoints/colbert-batch_3 ..\n",
      "[Oct 26, 08:51:26] name:/tmp/tmptrqmqrf1/output_dir/test_training_teacher/2022-10/26/08.43.55/checkpoints/colbert-LAST.dnn\n",
      "[Oct 26, 08:51:26] Make a sym link of /tmp/tmptrqmqrf1/output_dir/test_training_teacher/2022-10/26/08.43.55/checkpoints/colbert.dnn.batch_3.model to /tmp/tmptrqmqrf1/output_dir/test_training_teacher/2022-10/26/08.43.55/checkpoints/colbert-LAST.dnn\n",
      "[Oct 26, 08:51:26] #> Done with all triples!\n",
      "#> Saving a checkpoint to /tmp/tmptrqmqrf1/output_dir/test_training_teacher/2022-10/26/08.43.55/checkpoints/colbert ..\n"
     ]
    }
   ],
   "source": [
    "with Run().context(RunConfig(root=args_dict['root'], experiment=args_dict['experiment'], nranks=args_dict['nranks'], amp=args_dict['amp'])):\n",
    "    colBERTConfig = ColBERTConfig(**args_dict)   \n",
    "    teacher_model_fn = train(colBERTConfig, text_triples_en_fn, None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fa75ee",
   "metadata": {},
   "source": [
    "In the following two Knowledge Distillation (KD) steps, we will train the student model using \n",
    "1. parallel data contatining a) English and non-English passages for the student model, and b) English passages for the teacher model, \n",
    "2. parallel data containg a) non-English queries and English passages for the student model, and b) English queries and English passages for the student model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee07ecc9",
   "metadata": {},
   "source": [
    "First, we run Knowledge Distillation where the student learns teacher's token representations using the parallel English and non-English passage data from the following two files: \n",
    "\n",
    "a) English and non-English passages\n",
    "\n",
    "b) English only passages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10832ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_non_en_fn = os.path.join(test_files_location, \"7lan_notrim_triple_2ep.other.clean.h5\")\n",
    "parallel_en_fn = os.path.join(test_files_location, \"7lan_notrim_triple_2ep.en.clean.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84268e12",
   "metadata": {},
   "source": [
    "Here is an example item from the mixed-language file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e01878d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>সে একটা জরুরি পুনর্বাসনের কাজ করছিল</td>\n",
       "      <td>She was right in the middle of an important piece of restoration .</td>\n",
       "      <td>She was right in the middle of an important piece of restoration .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv(parallel_non_en_fn, sep='\\t', nrows=1, header=None)\n",
    "display(HTML(data.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc14bf4",
   "metadata": {},
   "source": [
    "Here are the parameters and command for the first Knowledge Distillation (KD) stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1335cf78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"index_path\": null,\n",
      "    \"index_location\": null,\n",
      "    \"nbits\": 1,\n",
      "    \"kmeans_niters\": 20,\n",
      "    \"num_partitions_max\": 10000000,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 1,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 3e-6,\n",
      "    \"maxsteps\": 3,\n",
      "    \"save_every\": null,\n",
      "    \"resume\": false,\n",
      "    \"resume_optimizer\": false,\n",
      "    \"warmup\": null,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 2,\n",
      "    \"use_ib_negatives\": false,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"shuffle_every_epoch\": false,\n",
      "    \"save_steps\": 2000,\n",
      "    \"save_epochs\": -1,\n",
      "    \"epochs\": 1,\n",
      "    \"input_arguments\": {},\n",
      "    \"model_type\": \"xlm-roberta-base\",\n",
      "    \"init_from_lm\": null,\n",
      "    \"local_models_repository\": null,\n",
      "    \"ranks_fn\": null,\n",
      "    \"output_dir\": null,\n",
      "    \"topK\": 100,\n",
      "    \"student_teacher_temperature\": 1.0,\n",
      "    \"student_teacher_top_loss_weight\": 0.5,\n",
      "    \"teacher_model_type\": \"xlm-roberta-base\",\n",
      "    \"teacher_doc_maxlen\": 180,\n",
      "    \"distill_query_passage_separately\": true,\n",
      "    \"query_only\": false,\n",
      "    \"loss_function\": null,\n",
      "    \"query_weight\": 0.5,\n",
      "    \"rng_seed\": 12345,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 128,\n",
      "    \"doc_maxlen\": 180,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"\\/tmp\\/tmptrqmqrf1\\/output_dir\\/test_training_student\\/2022-10\\/26\\/08.43.55\\/checkpoints\\/colbert-LAST.dnn\",\n",
      "    \"teacher_checkpoint\": \"\\/tmp\\/tmptrqmqrf1\\/output_dir\\/test_training_teacher\\/2022-10\\/26\\/08.43.55\\/checkpoints\\/colbert-LAST.dnn\",\n",
      "    \"triples\": \"..\\/..\\/..\\/tests\\/resources\\/ir_dense\\/7lan_notrim_triple_2ep.other.clean.h5\",\n",
      "    \"teacher_triples\": \"..\\/..\\/..\\/tests\\/resources\\/ir_dense\\/7lan_notrim_triple_2ep.en.clean.h5\",\n",
      "    \"collection\": null,\n",
      "    \"queries\": null,\n",
      "    \"index_name\": null,\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\\/tmp\\/tmptrqmqrf1\\/output_dir\",\n",
      "    \"experiment\": \"test_training\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2022-10\\/26\\/08.43.55\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 0\n",
      "}\n",
      "[Oct 26, 08:51:34] model type: xlm-roberta-base\n",
      "[Oct 26, 08:51:36] Using config.bsize = 1 (per process) and config.accumsteps = 1\n",
      "[Oct 26, 08:51:36] get query model type: xlm-roberta-base\n",
      "[Oct 26, 08:51:40] get doc model type: xlm-roberta-base\n",
      "[Oct 26, 08:51:42] #> Loading triples...\n",
      "[Oct 26, 08:51:42] get query model type: xlm-roberta-base\n",
      "[Oct 26, 08:51:44] get doc model type: xlm-roberta-base\n",
      "[Oct 26, 08:51:46] #> Loading triples...\n",
      "[Oct 26, 08:51:46] #>>>>> at ColBERT name (model type) : xlm-roberta-base\n",
      "[Oct 26, 08:51:46] #>>>>> at BaseColBERT name (model type) : xlm-roberta-base\n",
      "[Oct 26, 08:51:46] #> base_config.py load_from_checkpoint xlm-roberta-base\n",
      "[Oct 26, 08:51:46] #> base_config.py load_from_checkpoint xlm-roberta-base/artifact.metadata\n",
      "[Oct 26, 08:51:46] factory model type: xlm-roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing HF_ColBERT_XLMR: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing HF_ColBERT_XLMR from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing HF_ColBERT_XLMR from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of HF_ColBERT_XLMR were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.pooler.dense.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.pooler.dense.bias', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.output.dense.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'linear.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.5.output.LayerNorm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 26, 08:52:07] #> Starting from checkpoint /tmp/tmptrqmqrf1/output_dir/test_training_student/2022-10/26/08.43.55/checkpoints/colbert-LAST.dnn\n",
      "[Oct 26, 08:52:15] #>>>>> at ColBERT name (model type) : xlm-roberta-base\n",
      "[Oct 26, 08:52:15] #>>>>> at BaseColBERT name (model type) : xlm-roberta-base\n",
      "[Oct 26, 08:52:15] #> base_config.py load_from_checkpoint xlm-roberta-base\n",
      "[Oct 26, 08:52:15] #> base_config.py load_from_checkpoint xlm-roberta-base/artifact.metadata\n",
      "[Oct 26, 08:52:15] factory model type: xlm-roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing HF_ColBERT_XLMR: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing HF_ColBERT_XLMR from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing HF_ColBERT_XLMR from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of HF_ColBERT_XLMR were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.pooler.dense.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.pooler.dense.bias', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.output.dense.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'linear.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.5.output.LayerNorm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 26, 08:52:35] #> Loading teacher checkpoint /tmp/tmptrqmqrf1/output_dir/test_training_teacher/2022-10/26/08.43.55/checkpoints/colbert-LAST.dnn\n",
      "[Oct 26, 08:52:40] distill_query_passage_separately functionality is not supported (yet)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/franzm/packages/minconda3/envs/primeqa_pyarrow/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 26, 08:52:41] maxsteps: 3\n",
      "[Oct 26, 08:52:41] 1 epochs of 5 examples\n",
      "[Oct 26, 08:52:41] batch size: 1\n",
      "[Oct 26, 08:52:41] maxsteps set to 3\n",
      "[Oct 26, 08:52:41] start batch idx: 0\n",
      "[Oct 26, 08:52:41] #> XMLR QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "[Oct 26, 08:52:41] #> Input: $ সে একটা জরুরি পুনর্বাসনের কাজ করছিল, \t\t True, \t\t None\n",
      "[Oct 26, 08:52:41] #> Output IDs: torch.Size([32]), tensor([     0,   9748,  22540,  71896, 237528, 231247,  71994,  31938,  30234,\n",
      "        168033,   2763,      2,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1])\n",
      "[Oct 26, 08:52:41] #> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "[Oct 26, 08:52:41] #> XLMR DocTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "[Oct 26, 08:52:41] #> Input: $ She was right in the middle of an important piece of restoration ., \t\t None\n",
      "[Oct 26, 08:52:41] #> Output IDs: torch.Size([18]), tensor([    0,  9749,  4687,   509,  7108,    23,    70, 86991,   111,   142,\n",
      "         5526, 63847,   111, 14359, 30494,     6,     5,     2])\n",
      "[Oct 26, 08:52:41] #> Output Mask: torch.Size([18]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "[Oct 26, 08:52:41] #> XMLR QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "[Oct 26, 08:52:41] #> Input: $ She was right in the middle of an important piece of restoration ., \t\t True, \t\t None\n",
      "[Oct 26, 08:52:41] #> Output IDs: torch.Size([32]), tensor([    0,  9748,  4687,   509,  7108,    23,    70, 86991,   111,   142,\n",
      "         5526, 63847,   111, 14359, 30494,     6,     5,     2,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1])\n",
      "[Oct 26, 08:52:41] #> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "[Oct 26, 08:52:41] #> XLMR DocTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "[Oct 26, 08:52:41] #> Input: $ She was right in the middle of an important piece of restoration ., \t\t None\n",
      "[Oct 26, 08:52:41] #> Output IDs: torch.Size([18]), tensor([    0,  9749,  4687,   509,  7108,    23,    70, 86991,   111,   142,\n",
      "         5526, 63847,   111, 14359, 30494,     6,     5,     2])\n",
      "[Oct 26, 08:52:41] #> Output Mask: torch.Size([18]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "[Oct 26, 08:52:41] #>>>> colbert query ==\n",
      "[Oct 26, 08:52:41] #>>>>> input_ids: torch.Size([32]), tensor([     0,   9748,  22540,  71896, 237528, 231247,  71994,  31938,  30234,\n",
      "        168033,   2763,      2,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/franzm/packages/minconda3/envs/primeqa_pyarrow/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "/u/franzm/packages/minconda3/envs/primeqa_pyarrow/lib/python3.9/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 26, 08:52:45] #>>>> before linear query ==\n",
      "[Oct 26, 08:52:45] #>>>>> Q: torch.Size([32, 768]), tensor([[-1.5845e-03,  7.9839e-02,  5.0856e-02,  ..., -1.0550e-01,\n",
      "          9.7534e-02,  1.7295e-02],\n",
      "        [-8.6329e-03,  2.0453e-02,  4.7944e-05,  ...,  4.7402e-02,\n",
      "          8.4028e-02,  4.5498e-02],\n",
      "        [-9.8698e-03,  8.0432e-02, -2.1066e-02,  ..., -1.1526e-01,\n",
      "          1.0836e-02,  6.8963e-02],\n",
      "        ...,\n",
      "        [ 1.9038e-02,  8.9034e-02,  4.3189e-02,  ..., -2.7036e-02,\n",
      "          5.8573e-02, -2.5946e-02],\n",
      "        [ 6.7096e-02,  9.8762e-02,  1.1187e-02,  ..., -4.2818e-02,\n",
      "          5.3878e-02,  4.3672e-02],\n",
      "        [ 7.1772e-02,  9.2112e-02,  8.6087e-03,  ..., -1.7460e-01,\n",
      "          6.0347e-02,  4.0607e-03]], grad_fn=<SelectBackward0>)\n",
      "[Oct 26, 08:52:45] #>>>>> self.linear query : Parameter containing:\n",
      "tensor([[-1.2620e-02, -3.6884e-03, -1.0780e-02,  ..., -1.3896e-02,\n",
      "          1.2237e-02, -8.2247e-03],\n",
      "        [-3.5070e-03,  3.9557e-03, -2.4896e-02,  ..., -2.6217e-02,\n",
      "          4.0822e-03, -8.0775e-03],\n",
      "        [-1.1149e-02, -1.4444e-05,  1.5010e-02,  ..., -1.1314e-02,\n",
      "         -9.3701e-03, -2.2194e-02],\n",
      "        ...,\n",
      "        [-1.0288e-02,  3.2584e-02,  3.3388e-03,  ...,  3.1875e-02,\n",
      "          3.6858e-03,  2.2995e-02],\n",
      "        [ 1.5650e-03,  2.2251e-03,  1.3353e-02,  ..., -2.8309e-02,\n",
      "          3.9367e-03,  1.1989e-02],\n",
      "        [-5.5845e-03,  3.1916e-02, -2.3427e-02,  ..., -1.1540e-04,\n",
      "          1.1096e-02, -9.7595e-03]], requires_grad=True)\n",
      "[Oct 26, 08:52:45] #>>>> colbert query ==\n",
      "[Oct 26, 08:52:45] #>>>>> Q: torch.Size([32, 128]), tensor([[-0.2041, -0.0016, -0.3724,  ..., -0.1747, -0.1431, -0.1762],\n",
      "        [-0.0569,  0.2195, -0.5346,  ..., -0.3688, -0.2706, -0.0456],\n",
      "        [-0.0412,  0.2009, -0.5407,  ..., -0.3379, -0.2852, -0.0327],\n",
      "        ...,\n",
      "        [-0.1182,  0.1980, -0.4752,  ..., -0.2511, -0.3005, -0.1150],\n",
      "        [-0.1967,  0.0013, -0.3651,  ..., -0.1890, -0.1514, -0.1907],\n",
      "        [-0.1939,  0.0032, -0.3468,  ..., -0.1831, -0.1217, -0.1843]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "[Oct 26, 08:52:45] #>>>> colbert doc ==\n",
      "[Oct 26, 08:52:45] #>>>>> input_ids: torch.Size([18]), tensor([    0,  9749,  4687,   509,  7108,    23,    70, 86991,   111,   142,\n",
      "         5526, 63847,   111, 14359, 30494,     6,     5,     2])\n",
      "[Oct 26, 08:52:50] #>>>> before linear doc ==\n",
      "[Oct 26, 08:52:50] #>>>>> D: torch.Size([18, 768]), tensor([[ 0.0894,  0.0226,  0.0727,  ..., -0.1267,  0.1134,  0.0495],\n",
      "        [-0.0838,  0.0573, -0.0342,  ..., -0.0132, -0.0189,  0.1143],\n",
      "        [-0.0457,  0.0822,  0.0165,  ..., -0.0737,  0.0391, -0.0255],\n",
      "        ...,\n",
      "        [-0.0248,  0.0590,  0.0018,  ...,  0.0723,  0.0868, -0.0758],\n",
      "        [-0.0114,  0.0556,  0.0132,  ...,  0.0245,  0.1273,  0.0049],\n",
      "        [ 0.2028,  0.0588,  0.0307,  ..., -0.3938,  0.0891,  0.0995]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "[Oct 26, 08:52:50] #>>>>> self.linear doc : Parameter containing:\n",
      "tensor([[-1.2620e-02, -3.6884e-03, -1.0780e-02,  ..., -1.3896e-02,\n",
      "          1.2237e-02, -8.2247e-03],\n",
      "        [-3.5070e-03,  3.9557e-03, -2.4896e-02,  ..., -2.6217e-02,\n",
      "          4.0822e-03, -8.0775e-03],\n",
      "        [-1.1149e-02, -1.4444e-05,  1.5010e-02,  ..., -1.1314e-02,\n",
      "         -9.3701e-03, -2.2194e-02],\n",
      "        ...,\n",
      "        [-1.0288e-02,  3.2584e-02,  3.3388e-03,  ...,  3.1875e-02,\n",
      "          3.6858e-03,  2.2995e-02],\n",
      "        [ 1.5650e-03,  2.2251e-03,  1.3353e-02,  ..., -2.8309e-02,\n",
      "          3.9367e-03,  1.1989e-02],\n",
      "        [-5.5845e-03,  3.1916e-02, -2.3427e-02,  ..., -1.1540e-04,\n",
      "          1.1096e-02, -9.7595e-03]], requires_grad=True)\n",
      "[Oct 26, 08:52:50] #>>>> colbert doc ==\n",
      "[Oct 26, 08:52:50] #>>>>> D: torch.Size([18, 128]), tensor([[-0.2291, -0.0367, -0.3479,  ..., -0.0985, -0.1030, -0.1944],\n",
      "        [-0.0440,  0.1381, -0.4301,  ..., -0.2368, -0.2357, -0.0352],\n",
      "        [-0.1360,  0.1735, -0.4587,  ..., -0.2270, -0.2122, -0.0810],\n",
      "        ...,\n",
      "        [-0.1220,  0.0938, -0.4754,  ..., -0.1882, -0.3163, -0.0788],\n",
      "        [-0.1825,  0.1336, -0.4926,  ..., -0.1708, -0.2842, -0.1219],\n",
      "        [-0.2817, -0.0774, -0.2323,  ...,  0.0242, -0.0087, -0.2354]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "[Oct 26, 08:52:50] #>>>> colbert query ==\n",
      "[Oct 26, 08:52:50] #>>>>> input_ids: torch.Size([32]), tensor([    0,  9748,  4687,   509,  7108,    23,    70, 86991,   111,   142,\n",
      "         5526, 63847,   111, 14359, 30494,     6,     5,     2,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1])\n",
      "[Oct 26, 08:52:54] #>>>> before linear query ==\n",
      "[Oct 26, 08:52:54] #>>>>> Q: torch.Size([32, 768]), tensor([[ 0.2680,  0.0579,  0.1197,  ..., -0.2949,  0.1929, -0.0099],\n",
      "        [-0.0314,  0.0527, -0.0037,  ..., -0.0668, -0.0142,  0.0595],\n",
      "        [-0.0380,  0.0557,  0.0114,  ...,  0.0641,  0.0470, -0.0290],\n",
      "        ...,\n",
      "        [ 0.3376, -0.0019, -0.1732,  ..., -0.9174, -0.1352,  0.2070],\n",
      "        [ 0.3376, -0.0019, -0.1732,  ..., -0.9174, -0.1352,  0.2070],\n",
      "        [ 0.3376, -0.0019, -0.1732,  ..., -0.9174, -0.1352,  0.2070]])\n",
      "[Oct 26, 08:52:54] #>>>>> self.linear query : Parameter containing:\n",
      "tensor([[-1.2613e-02, -3.6825e-03, -1.0767e-02,  ..., -1.3904e-02,\n",
      "          1.2236e-02, -8.2245e-03],\n",
      "        [-3.5072e-03,  3.9647e-03, -2.4897e-02,  ..., -2.6205e-02,\n",
      "          4.0816e-03, -8.0879e-03],\n",
      "        [-1.1153e-02, -2.5158e-05,  1.5023e-02,  ..., -1.1317e-02,\n",
      "         -9.3637e-03, -2.2197e-02],\n",
      "        ...,\n",
      "        [-1.0294e-02,  3.2583e-02,  3.3276e-03,  ...,  3.1877e-02,\n",
      "          3.6833e-03,  2.3003e-02],\n",
      "        [ 1.5650e-03,  2.2403e-03,  1.3361e-02,  ..., -2.8309e-02,\n",
      "          3.9306e-03,  1.1987e-02],\n",
      "        [-5.5885e-03,  3.1930e-02, -2.3424e-02,  ..., -1.1074e-04,\n",
      "          1.1096e-02, -9.7742e-03]], requires_grad=True)\n",
      "[Oct 26, 08:52:54] #>>>> colbert query ==\n",
      "[Oct 26, 08:52:54] #>>>>> Q: torch.Size([32, 128]), tensor([[-0.3104, -0.1458, -0.2599,  ...,  0.0150,  0.0021, -0.2681],\n",
      "        [-0.0406,  0.2060, -0.4712,  ..., -0.1590, -0.2777, -0.0117],\n",
      "        [-0.1144,  0.2498, -0.4670,  ..., -0.2172, -0.2470, -0.0257],\n",
      "        ...,\n",
      "        [-0.1256, -0.1595, -0.1716,  ...,  0.0768,  0.2064, -0.3160],\n",
      "        [-0.1256, -0.1595, -0.1716,  ...,  0.0768,  0.2064, -0.3160],\n",
      "        [-0.1256, -0.1595, -0.1716,  ...,  0.0768,  0.2064, -0.3160]])\n",
      "[Oct 26, 08:52:54] #>>>> colbert doc ==\n",
      "[Oct 26, 08:52:54] #>>>>> input_ids: torch.Size([18]), tensor([    0,  9749,  4687,   509,  7108,    23,    70, 86991,   111,   142,\n",
      "         5526, 63847,   111, 14359, 30494,     6,     5,     2])\n",
      "[Oct 26, 08:52:58] #>>>> before linear doc ==\n",
      "[Oct 26, 08:52:58] #>>>>> D: torch.Size([18, 768]), tensor([[ 0.2506,  0.0487,  0.1312,  ..., -0.2775,  0.1999, -0.0037],\n",
      "        [-0.0387,  0.0488,  0.0064,  ..., -0.0754, -0.0225,  0.0701],\n",
      "        [-0.0351,  0.0597,  0.0127,  ...,  0.0329,  0.0526, -0.0227],\n",
      "        ...,\n",
      "        [-0.0403,  0.0569, -0.0207,  ...,  0.0671,  0.1131, -0.0544],\n",
      "        [ 0.0159,  0.0940, -0.0244,  ..., -0.0677,  0.0985, -0.0356],\n",
      "        [ 0.3187, -0.0127, -0.1306,  ..., -0.8657, -0.0987,  0.2051]])\n",
      "[Oct 26, 08:52:58] #>>>>> self.linear doc : Parameter containing:\n",
      "tensor([[-1.2613e-02, -3.6825e-03, -1.0767e-02,  ..., -1.3904e-02,\n",
      "          1.2236e-02, -8.2245e-03],\n",
      "        [-3.5072e-03,  3.9647e-03, -2.4897e-02,  ..., -2.6205e-02,\n",
      "          4.0816e-03, -8.0879e-03],\n",
      "        [-1.1153e-02, -2.5158e-05,  1.5023e-02,  ..., -1.1317e-02,\n",
      "         -9.3637e-03, -2.2197e-02],\n",
      "        ...,\n",
      "        [-1.0294e-02,  3.2583e-02,  3.3276e-03,  ...,  3.1877e-02,\n",
      "          3.6833e-03,  2.3003e-02],\n",
      "        [ 1.5650e-03,  2.2403e-03,  1.3361e-02,  ..., -2.8309e-02,\n",
      "          3.9306e-03,  1.1987e-02],\n",
      "        [-5.5885e-03,  3.1930e-02, -2.3424e-02,  ..., -1.1074e-04,\n",
      "          1.1096e-02, -9.7742e-03]], requires_grad=True)\n",
      "[Oct 26, 08:52:58] #>>>> colbert doc ==\n",
      "[Oct 26, 08:52:58] #>>>>> D: torch.Size([18, 128]), tensor([[-0.3045, -0.1379, -0.2729,  ..., -0.0039, -0.0137, -0.2609],\n",
      "        [-0.0246,  0.1901, -0.4688,  ..., -0.1680, -0.2688, -0.0200],\n",
      "        [-0.1119,  0.2409, -0.4636,  ..., -0.2244, -0.2504, -0.0329],\n",
      "        ...,\n",
      "        [-0.1333,  0.0861, -0.5075,  ..., -0.1814, -0.3213, -0.0695],\n",
      "        [-0.1911,  0.1594, -0.5884,  ..., -0.2044, -0.2783, -0.0836],\n",
      "        [-0.1356, -0.1544, -0.1928,  ...,  0.0560,  0.1754, -0.3100]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#>>>    29.24 29.53 \t\t|\t\t -0.2900000000000027\n",
      "[Oct 26, 08:53:39] 0 0.021523209288716316\n",
      "#>>>    26.44 28.06 \t\t|\t\t -1.6199999999999974\n",
      "[Oct 26, 08:54:39] 1 0.021525634886696933\n",
      "#>>>    29.57 28.82 \t\t|\t\t 0.75\n",
      "[Oct 26, 08:55:30] 2 0.021523664447976276\n",
      "#> Saving a checkpoint to /tmp/tmptrqmqrf1/output_dir/test_training/2022-10/26/08.43.55/checkpoints/colbert.dnn.batch_3.model ..\n",
      "#> Saving a checkpoint to /tmp/tmptrqmqrf1/output_dir/test_training/2022-10/26/08.43.55/checkpoints/colbert-batch_3 ..\n",
      "[Oct 26, 08:55:43] name:/tmp/tmptrqmqrf1/output_dir/test_training/2022-10/26/08.43.55/checkpoints/colbert-LAST.dnn\n",
      "[Oct 26, 08:55:43] Make a sym link of /tmp/tmptrqmqrf1/output_dir/test_training/2022-10/26/08.43.55/checkpoints/colbert.dnn.batch_3.model to /tmp/tmptrqmqrf1/output_dir/test_training/2022-10/26/08.43.55/checkpoints/colbert-LAST.dnn\n",
      "[Oct 26, 08:55:43] #> Done with all triples!\n",
      "#> Saving a checkpoint to /tmp/tmptrqmqrf1/output_dir/test_training/2022-10/26/08.43.55/checkpoints/colbert ..\n"
     ]
    }
   ],
   "source": [
    "parallel_non_en_fn = os.path.join(test_files_location, \"7lan_notrim_triple_2ep.other.clean.h5\")\n",
    "parallel_en_fn = os.path.join(test_files_location, \"7lan_notrim_triple_2ep.en.clean.h5\")\n",
    "\n",
    "args_dict = {\n",
    "    'root': output_dir,\n",
    "    'experiment': 'test_training',\n",
    "    'model_type': model_type,\n",
    "    'checkpoint': student_model_fn + '-LAST.dnn',\n",
    "    'distill_query_passage_separately': True,\n",
    "    'teacher_model_type': model_type,\n",
    "    'teacher_checkpoint': teacher_model_fn + '-LAST.dnn',\n",
    "    'triples': parallel_non_en_fn,\n",
    "    'teacher_triples': parallel_en_fn,\n",
    "    'maxsteps': 3,\n",
    "    'bsize': 1,\n",
    "    'accumsteps': 1,\n",
    "    'amp': True,\n",
    "    'epochs': 1,\n",
    "    'rank': 0,\n",
    "    'nranks': 1\n",
    "}\n",
    "\n",
    "with Run().context(RunConfig(root=args_dict['root'], experiment=args_dict['experiment'], nranks=args_dict['nranks'], amp=args_dict['amp'])):\n",
    "    colBERTConfig = ColBERTConfig(**args_dict)\n",
    "    stage_1_student_model_fn = train(colBERTConfig, parallel_non_en_fn, None, None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7799c7cf",
   "metadata": {},
   "source": [
    "Next, we run Knowledge Distillation where the student learns teacher's relavance prediction score using parallel data containing:\n",
    "\n",
    "a) non-English queries and English passages for the student model\n",
    "\n",
    "b) English queries and English passages for the teacher model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "753b46d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"index_path\": null,\n",
      "    \"index_location\": null,\n",
      "    \"nbits\": 1,\n",
      "    \"kmeans_niters\": 20,\n",
      "    \"num_partitions_max\": 10000000,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 1,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 3e-6,\n",
      "    \"maxsteps\": 3,\n",
      "    \"save_every\": null,\n",
      "    \"resume\": false,\n",
      "    \"resume_optimizer\": false,\n",
      "    \"warmup\": null,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 2,\n",
      "    \"use_ib_negatives\": false,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"shuffle_every_epoch\": false,\n",
      "    \"save_steps\": 2000,\n",
      "    \"save_epochs\": -1,\n",
      "    \"epochs\": 1,\n",
      "    \"input_arguments\": {},\n",
      "    \"model_type\": \"xlm-roberta-base\",\n",
      "    \"init_from_lm\": null,\n",
      "    \"local_models_repository\": null,\n",
      "    \"ranks_fn\": null,\n",
      "    \"output_dir\": null,\n",
      "    \"topK\": 100,\n",
      "    \"student_teacher_temperature\": 1.0,\n",
      "    \"student_teacher_top_loss_weight\": 0.5,\n",
      "    \"teacher_model_type\": \"xlm-roberta-base\",\n",
      "    \"teacher_doc_maxlen\": 180,\n",
      "    \"distill_query_passage_separately\": false,\n",
      "    \"query_only\": false,\n",
      "    \"loss_function\": null,\n",
      "    \"query_weight\": 0.5,\n",
      "    \"rng_seed\": 12345,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 128,\n",
      "    \"doc_maxlen\": 180,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"\\/tmp\\/tmptrqmqrf1\\/output_dir\\/test_training\\/2022-10\\/26\\/08.43.55\\/checkpoints\\/colbert-LAST.dnn\",\n",
      "    \"teacher_checkpoint\": \"\\/tmp\\/tmptrqmqrf1\\/output_dir\\/test_training_teacher\\/2022-10\\/26\\/08.43.55\\/checkpoints\\/colbert-LAST.dnn\",\n",
      "    \"triples\": \"..\\/..\\/..\\/tests\\/resources\\/ir_dense\\/xorqa.train_ir_negs_5_poss_1_001pct_at_0pct_en.tsv\",\n",
      "    \"teacher_triples\": \"..\\/..\\/..\\/tests\\/resources\\/ir_dense\\/xorqa.train_ir_negs_5_poss_1_001pct_at_0pct_en.tsv\",\n",
      "    \"collection\": null,\n",
      "    \"queries\": null,\n",
      "    \"index_name\": null,\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\\/tmp\\/tmptrqmqrf1\\/output_dir\",\n",
      "    \"experiment\": \"test_training\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2022-10\\/26\\/08.43.55\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 0\n",
      "}\n",
      "[Oct 26, 09:02:00] model type: xlm-roberta-base\n",
      "[Oct 26, 09:02:01] Using config.bsize = 1 (per process) and config.accumsteps = 1\n",
      "[Oct 26, 09:02:01] get query model type: xlm-roberta-base\n",
      "[Oct 26, 09:02:04] get doc model type: xlm-roberta-base\n",
      "[Oct 26, 09:02:07] #> Loading triples...\n",
      "[Oct 26, 09:02:07] get query model type: xlm-roberta-base\n",
      "[Oct 26, 09:02:08] get doc model type: xlm-roberta-base\n",
      "[Oct 26, 09:02:10] #> Loading triples...\n",
      "[Oct 26, 09:02:10] #>>>>> at ColBERT name (model type) : xlm-roberta-base\n",
      "[Oct 26, 09:02:10] #>>>>> at BaseColBERT name (model type) : xlm-roberta-base\n",
      "[Oct 26, 09:02:10] #> base_config.py load_from_checkpoint xlm-roberta-base\n",
      "[Oct 26, 09:02:10] #> base_config.py load_from_checkpoint xlm-roberta-base/artifact.metadata\n",
      "[Oct 26, 09:02:10] factory model type: xlm-roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing HF_ColBERT_XLMR: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing HF_ColBERT_XLMR from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing HF_ColBERT_XLMR from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of HF_ColBERT_XLMR were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.pooler.dense.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.pooler.dense.bias', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.output.dense.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'linear.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.5.output.LayerNorm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 26, 09:02:33] #> Starting from checkpoint /tmp/tmptrqmqrf1/output_dir/test_training/2022-10/26/08.43.55/checkpoints/colbert-LAST.dnn\n",
      "[Oct 26, 09:02:39] #>>>>> at ColBERT name (model type) : xlm-roberta-base\n",
      "[Oct 26, 09:02:39] #>>>>> at BaseColBERT name (model type) : xlm-roberta-base\n",
      "[Oct 26, 09:02:39] #> base_config.py load_from_checkpoint xlm-roberta-base\n",
      "[Oct 26, 09:02:39] #> base_config.py load_from_checkpoint xlm-roberta-base/artifact.metadata\n",
      "[Oct 26, 09:02:39] factory model type: xlm-roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing HF_ColBERT_XLMR: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing HF_ColBERT_XLMR from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing HF_ColBERT_XLMR from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of HF_ColBERT_XLMR were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.pooler.dense.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.pooler.dense.bias', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.output.dense.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'linear.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.5.output.LayerNorm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 26, 09:02:59] #> Loading teacher checkpoint /tmp/tmptrqmqrf1/output_dir/test_training_teacher/2022-10/26/08.43.55/checkpoints/colbert-LAST.dnn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/franzm/packages/minconda3/envs/primeqa_pyarrow/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 26, 09:03:05] maxsteps: 3\n",
      "[Oct 26, 09:03:05] 1 epochs of 5 examples\n",
      "[Oct 26, 09:03:05] batch size: 1\n",
      "[Oct 26, 09:03:05] maxsteps set to 3\n",
      "[Oct 26, 09:03:05] start batch idx: 0\n",
      "[Oct 26, 09:03:05] #> XMLR QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "[Oct 26, 09:03:05] #> Input: $ 중국에서 가장 오랜기간 왕위를 유지한 인물은 누구인가?, \t\t True, \t\t None\n",
      "[Oct 26, 09:03:05] #> Output IDs: torch.Size([32]), tensor([     0,   9748,  24120,   1180,  13968, 211059,  83639,  76826,  78363,\n",
      "         57104,    993, 161732,    697, 116932, 114150,     32,      2,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1])\n",
      "[Oct 26, 09:03:05] #> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "[Oct 26, 09:03:05] #> XLMR DocTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "[Oct 26, 09:03:05] #> Input: $ \"Kangxi Emperor The Kangxi Emperor's reign of 61 years makes him the longest-reigning emperor in Chinese history (although his grandson, the Qianlong Emperor, had the longest period of \"\"de facto\"\" power) and one of the longest-reigning rulers in the world. However, since he ascended the throne at the age of seven, actual power was held for six years by four regents and his grandmother, the Grand Empress Dowager Xiaozhuang.\", \t\t None\n",
      "[Oct 26, 09:03:05] #> Output IDs: torch.Size([155]), tensor([     0,   9749,     44,  50245, 122809,  31678,     56,    748,    581,\n",
      "         30267,   5134,  31678,     56,    748,     25,      7,   1690,  38529,\n",
      "           111,  11716,   5369,  30482,   4049,     70,   4989,    525,      9,\n",
      "           107,    872,    592,      6,  88940,    748,     23,  76438,  32692,\n",
      "            15,    289, 197271,   1919,  69190,    191,      4,     70,  43954,\n",
      "            66,  10617,  31678,     56,    748,      4,   1902,     70,   4989,\n",
      "           525,  14922,    111,     44,     58,    112,  72563,     58,     58,\n",
      "         14537,     16,    136,   1632,    111,     70,   4989,    525,      9,\n",
      "           107,    872,    592,  79986,   4295,     23,     70,   8999,      5,\n",
      "         33306,      4,  16792,    764,  82800,    297,     70,      6,  42294,\n",
      "            86,     99,     70,  32070,    111,  59671,      4,   8561,  14537,\n",
      "           509,  34658,    100,  37195,   5369,    390,  22759, 119555,    933,\n",
      "           136,   1919,   9963,    432,   9319,      4,     70,  12801,   2775,\n",
      "         11856, 145058,  33730, 211423,    169,   1132,   1463,   1242,      2,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1])\n",
      "[Oct 26, 09:03:05] #> Output Mask: torch.Size([155]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "[Oct 26, 09:03:05] #> XMLR QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "[Oct 26, 09:03:05] #> Input: $ Who maintained the throne for the longest time in China?, \t\t True, \t\t None\n",
      "[Oct 26, 09:03:05] #> Output IDs: torch.Size([32]), tensor([    0,  9748, 40469, 76104,   297,    70,     6, 42294,    86,   100,\n",
      "           70,  4989,   525,  1733,    23,  9098,    32,     2,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1])\n",
      "[Oct 26, 09:03:05] #> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "[Oct 26, 09:03:05] #> XLMR DocTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "[Oct 26, 09:03:05] #> Input: $ \"Kangxi Emperor The Kangxi Emperor's reign of 61 years makes him the longest-reigning emperor in Chinese history (although his grandson, the Qianlong Emperor, had the longest period of \"\"de facto\"\" power) and one of the longest-reigning rulers in the world. However, since he ascended the throne at the age of seven, actual power was held for six years by four regents and his grandmother, the Grand Empress Dowager Xiaozhuang.\", \t\t None\n",
      "[Oct 26, 09:03:05] #> Output IDs: torch.Size([155]), tensor([     0,   9749,     44,  50245, 122809,  31678,     56,    748,    581,\n",
      "         30267,   5134,  31678,     56,    748,     25,      7,   1690,  38529,\n",
      "           111,  11716,   5369,  30482,   4049,     70,   4989,    525,      9,\n",
      "           107,    872,    592,      6,  88940,    748,     23,  76438,  32692,\n",
      "            15,    289, 197271,   1919,  69190,    191,      4,     70,  43954,\n",
      "            66,  10617,  31678,     56,    748,      4,   1902,     70,   4989,\n",
      "           525,  14922,    111,     44,     58,    112,  72563,     58,     58,\n",
      "         14537,     16,    136,   1632,    111,     70,   4989,    525,      9,\n",
      "           107,    872,    592,  79986,   4295,     23,     70,   8999,      5,\n",
      "         33306,      4,  16792,    764,  82800,    297,     70,      6,  42294,\n",
      "            86,     99,     70,  32070,    111,  59671,      4,   8561,  14537,\n",
      "           509,  34658,    100,  37195,   5369,    390,  22759, 119555,    933,\n",
      "           136,   1919,   9963,    432,   9319,      4,     70,  12801,   2775,\n",
      "         11856, 145058,  33730, 211423,    169,   1132,   1463,   1242,      2,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1])\n",
      "[Oct 26, 09:03:05] #> Output Mask: torch.Size([155]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "[Oct 26, 09:03:05] #>>>> colbert query ==\n",
      "[Oct 26, 09:03:05] #>>>>> input_ids: torch.Size([32]), tensor([     0,   9748,  24120,   1180,  13968, 211059,  83639,  76826,  78363,\n",
      "         57104,    993, 161732,    697, 116932, 114150,     32,      2,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1])\n",
      "[Oct 26, 09:03:10] #>>>> before linear query ==\n",
      "[Oct 26, 09:03:10] #>>>>> Q: torch.Size([32, 768]), tensor([[ 0.0253,  0.0820,  0.0555,  ..., -0.0721,  0.0587,  0.0003],\n",
      "        [ 0.0766,  0.0117,  0.0447,  ..., -0.0914, -0.0511, -0.0400],\n",
      "        [ 0.0732,  0.0741, -0.0050,  ...,  0.0430,  0.0398,  0.1543],\n",
      "        ...,\n",
      "        [ 0.0505, -0.0105,  0.0388,  ..., -0.0666, -0.0480, -0.0362],\n",
      "        [ 0.1025,  0.0831,  0.0400,  ..., -0.0393,  0.0418,  0.0073],\n",
      "        [ 0.1261,  0.0689, -0.0196,  ..., -0.2266, -0.0304, -0.0359]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "[Oct 26, 09:03:10] #>>>>> self.linear query : Parameter containing:\n",
      "tensor([[-1.2608e-02, -3.6779e-03, -1.0781e-02,  ..., -1.3897e-02,\n",
      "          1.2241e-02, -8.2146e-03],\n",
      "        [-3.4997e-03,  3.9648e-03, -2.4895e-02,  ..., -2.6225e-02,\n",
      "          4.0922e-03, -8.0729e-03],\n",
      "        [-1.1159e-02, -1.7624e-05,  1.5027e-02,  ..., -1.1298e-02,\n",
      "         -9.3556e-03, -2.2198e-02],\n",
      "        ...,\n",
      "        [-1.0298e-02,  3.2585e-02,  3.3553e-03,  ...,  3.1883e-02,\n",
      "          3.6870e-03,  2.3002e-02],\n",
      "        [ 1.5565e-03,  2.2101e-03,  1.3369e-02,  ..., -2.8294e-02,\n",
      "          3.9525e-03,  1.1983e-02],\n",
      "        [-5.5793e-03,  3.1928e-02, -2.3419e-02,  ..., -1.2321e-04,\n",
      "          1.1101e-02, -9.7526e-03]], requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 26, 09:03:10] #>>>> colbert query ==\n",
      "[Oct 26, 09:03:10] #>>>>> Q: torch.Size([32, 128]), tensor([[-0.2179, -0.0212, -0.3695,  ..., -0.1597, -0.1289, -0.1689],\n",
      "        [-0.0519,  0.2481, -0.4561,  ..., -0.2290, -0.2685, -0.0693],\n",
      "        [-0.0309,  0.2590, -0.4800,  ..., -0.3211, -0.2496, -0.0094],\n",
      "        ...,\n",
      "        [-0.1647,  0.1821, -0.4240,  ..., -0.1822, -0.3135, -0.0908],\n",
      "        [-0.2180, -0.0300, -0.3580,  ..., -0.1671, -0.1416, -0.1824],\n",
      "        [-0.1909, -0.0392, -0.3386,  ..., -0.1474, -0.0767, -0.1944]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "[Oct 26, 09:03:10] #>>>> colbert doc ==\n",
      "[Oct 26, 09:03:10] #>>>>> input_ids: torch.Size([155]), tensor([     0,   9749,     44,  50245, 122809,  31678,     56,    748,    581,\n",
      "         30267,   5134,  31678,     56,    748,     25,      7,   1690,  38529,\n",
      "           111,  11716,   5369,  30482,   4049,     70,   4989,    525,      9,\n",
      "           107,    872,    592,      6,  88940,    748,     23,  76438,  32692,\n",
      "            15,    289, 197271,   1919,  69190,    191,      4,     70,  43954,\n",
      "            66,  10617,  31678,     56,    748,      4,   1902,     70,   4989,\n",
      "           525,  14922,    111,     44,     58,    112,  72563,     58,     58,\n",
      "         14537,     16,    136,   1632,    111,     70,   4989,    525,      9,\n",
      "           107,    872,    592,  79986,   4295,     23,     70,   8999,      5,\n",
      "         33306,      4,  16792,    764,  82800,    297,     70,      6,  42294,\n",
      "            86,     99,     70,  32070,    111,  59671,      4,   8561,  14537,\n",
      "           509,  34658,    100,  37195,   5369,    390,  22759, 119555,    933,\n",
      "           136,   1919,   9963,    432,   9319,      4,     70,  12801,   2775,\n",
      "         11856, 145058,  33730, 211423,    169,   1132,   1463,   1242,      2,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1])\n",
      "[Oct 26, 09:03:20] #>>>> before linear doc ==\n",
      "[Oct 26, 09:03:20] #>>>>> D: torch.Size([155, 768]), tensor([[ 1.0865e-01,  5.2918e-02,  7.2847e-02,  ..., -5.4294e-02,\n",
      "          4.3012e-02, -4.6923e-02],\n",
      "        [-4.1616e-02,  1.0328e-02, -2.2250e-02,  ..., -1.5369e-01,\n",
      "         -1.1544e-01,  3.6582e-02],\n",
      "        [-1.3947e-02,  2.1593e-02, -9.2714e-03,  ..., -1.0669e-02,\n",
      "          2.0114e-02,  4.9978e-02],\n",
      "        ...,\n",
      "        [ 4.7342e-04,  6.4813e-02,  2.3342e-02,  ...,  7.6956e-04,\n",
      "          5.1896e-03, -6.4069e-02],\n",
      "        [ 1.0320e-01,  7.1416e-02,  1.9522e-02,  ..., -7.8824e-03,\n",
      "         -4.7778e-02, -5.9810e-02],\n",
      "        [ 5.9682e-01,  1.6422e-01,  6.3937e-03,  ..., -7.5935e-01,\n",
      "         -1.7843e-01,  2.5045e-01]], grad_fn=<SelectBackward0>)\n",
      "[Oct 26, 09:03:20] #>>>>> self.linear doc : Parameter containing:\n",
      "tensor([[-1.2608e-02, -3.6779e-03, -1.0781e-02,  ..., -1.3897e-02,\n",
      "          1.2241e-02, -8.2146e-03],\n",
      "        [-3.4997e-03,  3.9648e-03, -2.4895e-02,  ..., -2.6225e-02,\n",
      "          4.0922e-03, -8.0729e-03],\n",
      "        [-1.1159e-02, -1.7624e-05,  1.5027e-02,  ..., -1.1298e-02,\n",
      "         -9.3556e-03, -2.2198e-02],\n",
      "        ...,\n",
      "        [-1.0298e-02,  3.2585e-02,  3.3553e-03,  ...,  3.1883e-02,\n",
      "          3.6870e-03,  2.3002e-02],\n",
      "        [ 1.5565e-03,  2.2101e-03,  1.3369e-02,  ..., -2.8294e-02,\n",
      "          3.9525e-03,  1.1983e-02],\n",
      "        [-5.5793e-03,  3.1928e-02, -2.3419e-02,  ..., -1.2321e-04,\n",
      "          1.1101e-02, -9.7526e-03]], requires_grad=True)\n",
      "[Oct 26, 09:03:20] #>>>> colbert doc ==\n",
      "[Oct 26, 09:03:20] #>>>>> D: torch.Size([155, 128]), tensor([[-0.2240, -0.0408, -0.3619,  ..., -0.0965, -0.1146, -0.2023],\n",
      "        [-0.0967,  0.2014, -0.4716,  ..., -0.1200, -0.2224, -0.0661],\n",
      "        [-0.0572,  0.2314, -0.4920,  ..., -0.2544, -0.2397, -0.0220],\n",
      "        ...,\n",
      "        [-0.0529,  0.2225, -0.4127,  ..., -0.2136, -0.3001,  0.0014],\n",
      "        [-0.1444, -0.0203, -0.3320,  ..., -0.1302, -0.1130, -0.1418],\n",
      "        [-0.2809, -0.1367, -0.0719,  ...,  0.3393,  0.3667, -0.4697]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "[Oct 26, 09:03:21] #>>>> colbert query ==\n",
      "[Oct 26, 09:03:21] #>>>>> input_ids: torch.Size([32]), tensor([    0,  9748, 40469, 76104,   297,    70,     6, 42294,    86,   100,\n",
      "           70,  4989,   525,  1733,    23,  9098,    32,     2,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1])\n",
      "[Oct 26, 09:03:24] #>>>> before linear query ==\n",
      "[Oct 26, 09:03:24] #>>>>> Q: torch.Size([32, 768]), tensor([[ 0.1430,  0.1339,  0.0530,  ..., -0.0602,  0.0759, -0.0409],\n",
      "        [-0.0103,  0.0793,  0.0221,  ..., -0.1324, -0.0900, -0.0563],\n",
      "        [-0.0138,  0.0994,  0.0200,  ...,  0.0477, -0.0294,  0.0660],\n",
      "        ...,\n",
      "        [ 0.1241,  0.1248, -0.0510,  ..., -0.2140, -0.0463,  0.0248],\n",
      "        [ 0.1241,  0.1248, -0.0510,  ..., -0.2140, -0.0463,  0.0248],\n",
      "        [ 0.1241,  0.1248, -0.0510,  ..., -0.2140, -0.0463,  0.0248]])\n",
      "[Oct 26, 09:03:24] #>>>>> self.linear query : Parameter containing:\n",
      "tensor([[-1.2613e-02, -3.6825e-03, -1.0767e-02,  ..., -1.3904e-02,\n",
      "          1.2236e-02, -8.2245e-03],\n",
      "        [-3.5072e-03,  3.9647e-03, -2.4897e-02,  ..., -2.6205e-02,\n",
      "          4.0816e-03, -8.0879e-03],\n",
      "        [-1.1153e-02, -2.5158e-05,  1.5023e-02,  ..., -1.1317e-02,\n",
      "         -9.3637e-03, -2.2197e-02],\n",
      "        ...,\n",
      "        [-1.0294e-02,  3.2583e-02,  3.3276e-03,  ...,  3.1877e-02,\n",
      "          3.6833e-03,  2.3003e-02],\n",
      "        [ 1.5650e-03,  2.2403e-03,  1.3361e-02,  ..., -2.8309e-02,\n",
      "          3.9306e-03,  1.1987e-02],\n",
      "        [-5.5885e-03,  3.1930e-02, -2.3424e-02,  ..., -1.1074e-04,\n",
      "          1.1096e-02, -9.7742e-03]], requires_grad=True)\n",
      "[Oct 26, 09:03:24] #>>>> colbert query ==\n",
      "[Oct 26, 09:03:24] #>>>>> Q: torch.Size([32, 128]), tensor([[-0.2492, -0.0616, -0.3449,  ..., -0.1061, -0.1020, -0.2218],\n",
      "        [-0.0838,  0.2290, -0.4673,  ..., -0.1539, -0.2270, -0.0501],\n",
      "        [-0.0423,  0.2647, -0.4273,  ..., -0.2459, -0.2102, -0.1604],\n",
      "        ...,\n",
      "        [-0.1787, -0.0477, -0.3400,  ..., -0.1208, -0.0649, -0.2229],\n",
      "        [-0.1787, -0.0477, -0.3400,  ..., -0.1208, -0.0649, -0.2229],\n",
      "        [-0.1787, -0.0477, -0.3400,  ..., -0.1208, -0.0649, -0.2229]])\n",
      "[Oct 26, 09:03:24] #>>>> colbert doc ==\n",
      "[Oct 26, 09:03:24] #>>>>> input_ids: torch.Size([155]), tensor([     0,   9749,     44,  50245, 122809,  31678,     56,    748,    581,\n",
      "         30267,   5134,  31678,     56,    748,     25,      7,   1690,  38529,\n",
      "           111,  11716,   5369,  30482,   4049,     70,   4989,    525,      9,\n",
      "           107,    872,    592,      6,  88940,    748,     23,  76438,  32692,\n",
      "            15,    289, 197271,   1919,  69190,    191,      4,     70,  43954,\n",
      "            66,  10617,  31678,     56,    748,      4,   1902,     70,   4989,\n",
      "           525,  14922,    111,     44,     58,    112,  72563,     58,     58,\n",
      "         14537,     16,    136,   1632,    111,     70,   4989,    525,      9,\n",
      "           107,    872,    592,  79986,   4295,     23,     70,   8999,      5,\n",
      "         33306,      4,  16792,    764,  82800,    297,     70,      6,  42294,\n",
      "            86,     99,     70,  32070,    111,  59671,      4,   8561,  14537,\n",
      "           509,  34658,    100,  37195,   5369,    390,  22759, 119555,    933,\n",
      "           136,   1919,   9963,    432,   9319,      4,     70,  12801,   2775,\n",
      "         11856, 145058,  33730, 211423,    169,   1132,   1463,   1242,      2,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1])\n",
      "[Oct 26, 09:03:32] #>>>> before linear doc ==\n",
      "[Oct 26, 09:03:32] #>>>>> D: torch.Size([155, 768]), tensor([[ 0.1005,  0.0909,  0.0866,  ..., -0.0505,  0.0522, -0.0426],\n",
      "        [-0.0518,  0.0138,  0.0013,  ..., -0.1404, -0.0883,  0.0109],\n",
      "        [-0.0147,  0.0238, -0.0356,  ...,  0.0298,  0.0207, -0.0013],\n",
      "        ...,\n",
      "        [ 0.0917,  0.0821,  0.0211,  ..., -0.1612, -0.0368,  0.0031],\n",
      "        [ 0.0917,  0.0821,  0.0211,  ..., -0.1612, -0.0368,  0.0031],\n",
      "        [ 0.0917,  0.0821,  0.0211,  ..., -0.1612, -0.0368,  0.0031]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 26, 09:03:32] #>>>>> self.linear doc : Parameter containing:\n",
      "tensor([[-1.2613e-02, -3.6825e-03, -1.0767e-02,  ..., -1.3904e-02,\n",
      "          1.2236e-02, -8.2245e-03],\n",
      "        [-3.5072e-03,  3.9647e-03, -2.4897e-02,  ..., -2.6205e-02,\n",
      "          4.0816e-03, -8.0879e-03],\n",
      "        [-1.1153e-02, -2.5158e-05,  1.5023e-02,  ..., -1.1317e-02,\n",
      "         -9.3637e-03, -2.2197e-02],\n",
      "        ...,\n",
      "        [-1.0294e-02,  3.2583e-02,  3.3276e-03,  ...,  3.1877e-02,\n",
      "          3.6833e-03,  2.3003e-02],\n",
      "        [ 1.5650e-03,  2.2403e-03,  1.3361e-02,  ..., -2.8309e-02,\n",
      "          3.9306e-03,  1.1987e-02],\n",
      "        [-5.5885e-03,  3.1930e-02, -2.3424e-02,  ..., -1.1074e-04,\n",
      "          1.1096e-02, -9.7742e-03]], requires_grad=True)\n",
      "[Oct 26, 09:03:32] #>>>> colbert doc ==\n",
      "[Oct 26, 09:03:32] #>>>>> D: torch.Size([155, 128]), tensor([[-0.2345, -0.0354, -0.3565,  ..., -0.1199, -0.1221, -0.1999],\n",
      "        [-0.0886,  0.2280, -0.5029,  ..., -0.1394, -0.2653, -0.0359],\n",
      "        [-0.0194,  0.2471, -0.5022,  ..., -0.2341, -0.2188,  0.0010],\n",
      "        ...,\n",
      "        [-0.1867, -0.0276, -0.3534,  ..., -0.1255, -0.0965, -0.2034],\n",
      "        [-0.1867, -0.0276, -0.3534,  ..., -0.1255, -0.0965, -0.2034],\n",
      "        [-0.1867, -0.0276, -0.3534,  ..., -0.1255, -0.0965, -0.2034]])\n",
      "#>>>    30.6 30.46 \t\t|\t\t 0.14000000000000057\n",
      "[Oct 26, 09:04:14] 0 0.0010372344404459\n",
      "#>>>    29.51 28.98 \t\t|\t\t 0.5300000000000011\n",
      "[Oct 26, 09:05:20] 1 0.0010668502431362868\n",
      "#>>>    30.15 29.96 \t\t|\t\t 0.18999999999999773\n",
      "[Oct 26, 09:06:23] 2 0.001067698154853657\n",
      "#> Saving a checkpoint to /tmp/tmptrqmqrf1/output_dir/test_training/2022-10/26/08.43.55/checkpoints/colbert.dnn.batch_3.model ..\n",
      "#> Saving a checkpoint to /tmp/tmptrqmqrf1/output_dir/test_training/2022-10/26/08.43.55/checkpoints/colbert-batch_3 ..\n",
      "[Oct 26, 09:06:58] name:/tmp/tmptrqmqrf1/output_dir/test_training/2022-10/26/08.43.55/checkpoints/colbert-LAST.dnn\n",
      "[Oct 26, 09:06:58] Make a sym link of /tmp/tmptrqmqrf1/output_dir/test_training/2022-10/26/08.43.55/checkpoints/colbert.dnn.batch_3.model to /tmp/tmptrqmqrf1/output_dir/test_training/2022-10/26/08.43.55/checkpoints/colbert-LAST.dnn\n",
      "[Oct 26, 09:06:58] #> Done with all triples!\n",
      "#> Saving a checkpoint to /tmp/tmptrqmqrf1/output_dir/test_training/2022-10/26/08.43.55/checkpoints/colbert ..\n"
     ]
    }
   ],
   "source": [
    "args_dict = {\n",
    "    'root': output_dir,\n",
    "    'experiment': 'test_training',\n",
    "    'model_type': model_type,\n",
    "    'checkpoint': stage_1_student_model_fn + '-LAST.dnn',\n",
    "    'distill_query_passage_separately': False,\n",
    "    'teacher_model_type': model_type,\n",
    "    'teacher_checkpoint': teacher_model_fn + '-LAST.dnn',\n",
    "    'triples': text_triples_en_fn,\n",
    "    'teacher_triples': text_triples_en_fn,\n",
    "    'maxsteps': 3,\n",
    "    'bsize': 1,\n",
    "    'accumsteps': 1,\n",
    "    'amp': True,\n",
    "    'epochs': 1,\n",
    "    'rank': 0,\n",
    "    'nranks': 1\n",
    "}\n",
    "\n",
    "with Run().context(RunConfig(root=args_dict['root'], experiment=args_dict['experiment'], nranks=args_dict['nranks'], amp=args_dict['amp'])):\n",
    "    colBERTConfig = ColBERTConfig(**args_dict)\n",
    "    stage_2_student_model_fn = train(colBERTConfig, text_triples_fn, None, None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff96db4",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "Next, we will index a collection of documents, using model representaion from the previous step. \n",
    "The collection is a TSV file, containing each document's ID, title, and text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7191fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_fn = os.path.join(test_files_location, \"xorqa.train_ir_001pct_at_0_pct_collection_fornum.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e662a2ea",
   "metadata": {},
   "source": [
    "Here is an example document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3be63d29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Kangxi Emperor's reign of 61 years makes him the longest-reigning emperor in Chinese history (although his grandson, the Qianlong Emperor, had the longest period of \"de facto\" power) and one of the longest-reigning rulers in the world. However, since he ascended the throne at the age of seven, actual power was held for six years by four regents and his grandmother, the Grand Empress Dowager Xiaozhuang.</td>\n",
       "      <td>Kangxi Emperor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv(collection_fn, sep='\\t', header=0, nrows=1)\n",
    "#data = pd.read_csv(collection_fn, sep='\\t', header=None, skiprows=3, nrows=1)\n",
    "display(HTML(data.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfadb75a",
   "metadata": {},
   "source": [
    "Here are the indexer arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "049c8272",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = {\n",
    "                'root': os.path.join(output_dir,'test_indexing'),\n",
    "                'experiment': 'test_indexing',\n",
    "                'checkpoint': stage_2_student_model_fn,\n",
    "                'collection': collection_fn,\n",
    "                'index_root': os.path.join(output_dir, 'test_indexing', 'indexes'),\n",
    "                'index_name': 'index_name',\n",
    "                'doc_maxlen': 180,\n",
    "                'num_partitions_max': 2,\n",
    "                'kmeans_niters': 1,\n",
    "                'nway': 1,\n",
    "                'rank': 0,\n",
    "                'nranks': 1,\n",
    "                'amp': True\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96f1dbe",
   "metadata": {},
   "source": [
    "Here we run the indexer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4078d9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[Oct 26, 09:01:04] #> Creating directory /tmp/tmptrqmqrf1/output_dir/test_indexing/indexes/index_name \n",
      "\n",
      "\n",
      "{\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"index_path\": null,\n",
      "    \"index_location\": null,\n",
      "    \"nbits\": 1,\n",
      "    \"kmeans_niters\": 1,\n",
      "    \"num_partitions_max\": 2,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 32,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 3e-6,\n",
      "    \"maxsteps\": 500000,\n",
      "    \"save_every\": null,\n",
      "    \"resume\": false,\n",
      "    \"resume_optimizer\": false,\n",
      "    \"warmup\": null,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 1,\n",
      "    \"use_ib_negatives\": false,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"shuffle_every_epoch\": false,\n",
      "    \"save_steps\": 2000,\n",
      "    \"save_epochs\": -1,\n",
      "    \"epochs\": 10,\n",
      "    \"input_arguments\": {},\n",
      "    \"model_type\": \"bert-base-uncased\",\n",
      "    \"init_from_lm\": null,\n",
      "    \"local_models_repository\": null,\n",
      "    \"ranks_fn\": null,\n",
      "    \"output_dir\": null,\n",
      "    \"topK\": 100,\n",
      "    \"student_teacher_temperature\": 1.0,\n",
      "    \"student_teacher_top_loss_weight\": 0.5,\n",
      "    \"teacher_model_type\": \"xlm-roberta-base\",\n",
      "    \"teacher_doc_maxlen\": 180,\n",
      "    \"distill_query_passage_separately\": false,\n",
      "    \"query_only\": false,\n",
      "    \"loss_function\": null,\n",
      "    \"query_weight\": 0.5,\n",
      "    \"rng_seed\": 12345,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 128,\n",
      "    \"doc_maxlen\": 180,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"\\/tmp\\/tmptrqmqrf1\\/output_dir\\/test_training\\/2022-10\\/26\\/08.43.55\\/checkpoints\\/colbert\",\n",
      "    \"teacher_checkpoint\": null,\n",
      "    \"triples\": null,\n",
      "    \"teacher_triples\": null,\n",
      "    \"collection\": \"..\\/..\\/..\\/tests\\/resources\\/ir_dense\\/xorqa.train_ir_001pct_at_0_pct_collection_fornum.tsv\",\n",
      "    \"queries\": null,\n",
      "    \"index_name\": \"index_name\",\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\\/tmp\\/tmptrqmqrf1\\/output_dir\\/test_indexing\",\n",
      "    \"experiment\": \"test_indexing\",\n",
      "    \"index_root\": \"\\/tmp\\/tmptrqmqrf1\\/output_dir\\/test_indexing\\/indexes\",\n",
      "    \"name\": \"2022-10\\/26\\/08.43.55\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 0\n",
      "}\n",
      "[Oct 26, 09:01:04] #> Loading collection...\n",
      "0M \n",
      "[Oct 26, 09:01:04] #>>>>> at ColBERT name (model type) : /tmp/tmptrqmqrf1/output_dir/test_training/2022-10/26/08.43.55/checkpoints/colbert\n",
      "[Oct 26, 09:01:04] #>>>>> at BaseColBERT name (model type) : /tmp/tmptrqmqrf1/output_dir/test_training/2022-10/26/08.43.55/checkpoints/colbert\n",
      "[Oct 26, 09:01:04] #> base_config.py load_from_checkpoint /tmp/tmptrqmqrf1/output_dir/test_training/2022-10/26/08.43.55/checkpoints/colbert\n",
      "[Oct 26, 09:01:04] #> base_config.py load_from_checkpoint /tmp/tmptrqmqrf1/output_dir/test_training/2022-10/26/08.43.55/checkpoints/colbert/artifact.metadata\n",
      "[Oct 26, 09:01:04] #> base_config.py from_path /tmp/tmptrqmqrf1/output_dir/test_training/2022-10/26/08.43.55/checkpoints/colbert/artifact.metadata\n",
      "[Oct 26, 09:01:04] #> base_config.py from_path args loaded! \n",
      "[Oct 26, 09:01:04] json file (get_colbert_from_pretrained): /tmp/tmptrqmqrf1/output_dir/test_training/2022-10/26/08.43.55/checkpoints/colbert/config.json\n",
      "[Oct 26, 09:01:04] factory model type: xlm-roberta-base\n",
      "[Oct 26, 09:01:18] json file (get_query_tokenizer): /tmp/tmptrqmqrf1/output_dir/test_training/2022-10/26/08.43.55/checkpoints/colbert/config.json\n",
      "[Oct 26, 09:01:18] get query model type: xlm-roberta-base\n",
      "[Oct 26, 09:01:21] json file (get_doc_tokenizer): /tmp/tmptrqmqrf1/output_dir/test_training/2022-10/26/08.43.55/checkpoints/colbert/config.json\n",
      "[Oct 26, 09:01:21] get doc model type: xlm-roberta-base\n",
      "[Oct 26, 09:01:23] [0] \t\t # of sampled PIDs = 7 \t sampled_pids[:3] = [3, 5, 0]\n",
      "[Oct 26, 09:01:23] [0] \t\t #> Encoding 7 passages..\n",
      "[Oct 26, 09:01:23] #> checkpoint, docFromText, Input: title | text, \t\t 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/franzm/packages/minconda3/envs/primeqa_pyarrow/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 26, 09:01:24] #> XLMR DocTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "[Oct 26, 09:01:24] #> Input: $ title | text, \t\t 32\n",
      "[Oct 26, 09:01:24] #> Output IDs: torch.Size([180]), tensor([    0,  9749, 44759,     6, 58745,  7986,     2,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1])\n",
      "[Oct 26, 09:01:24] #> Output Mask: torch.Size([180]), tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "[Oct 26, 09:01:24] #> checkpoint, docFromText, Output IDs: (tensor([[    0,  9749, 44759,  ...,     1,     1,     1],\n",
      "        [    0,  9749, 30267,  ...,     1,     1,     1],\n",
      "        [    0,  9749, 31678,  ...,     5,     2,     1],\n",
      "        ...,\n",
      "        [    0,  9749,  9098,  ...,     1,     1,     1],\n",
      "        [    0,  9749,   341,  ...,  4989,   525,     2],\n",
      "        [    0,  9749, 11617,  ...,     1,     1,     1]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]))\n",
      "[Oct 26, 09:01:25] #>>>> colbert doc ==\n",
      "[Oct 26, 09:01:25] #>>>>> input_ids: torch.Size([180]), tensor([    0,  9749, 44759,     6, 58745,  7986,     2,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/franzm/packages/minconda3/envs/primeqa_pyarrow/lib/python3.9/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 26, 09:01:35] #>>>> before linear doc ==\n",
      "[Oct 26, 09:01:35] #>>>>> D: torch.Size([180, 768]), tensor([[ 0.0680,  0.0969,  0.0546,  ..., -0.1383,  0.0549,  0.0015],\n",
      "        [ 0.0007,  0.0250,  0.0328,  ..., -0.1287,  0.0133,  0.1089],\n",
      "        [-0.0445,  0.0889, -0.0314,  ..., -0.2847, -0.0218,  0.0868],\n",
      "        ...,\n",
      "        [-0.0585,  0.0579,  0.0066,  ..., -0.1692, -0.0392, -0.0020],\n",
      "        [-0.0585,  0.0579,  0.0066,  ..., -0.1692, -0.0392, -0.0020],\n",
      "        [-0.0585,  0.0579,  0.0066,  ..., -0.1692, -0.0392, -0.0020]])\n",
      "[Oct 26, 09:01:35] #>>>>> self.linear doc : Parameter containing:\n",
      "tensor([[-1.2608e-02, -3.6779e-03, -1.0781e-02,  ..., -1.3897e-02,\n",
      "          1.2241e-02, -8.2146e-03],\n",
      "        [-3.4997e-03,  3.9648e-03, -2.4895e-02,  ..., -2.6225e-02,\n",
      "          4.0922e-03, -8.0729e-03],\n",
      "        [-1.1159e-02, -1.7624e-05,  1.5027e-02,  ..., -1.1298e-02,\n",
      "         -9.3556e-03, -2.2198e-02],\n",
      "        ...,\n",
      "        [-1.0298e-02,  3.2585e-02,  3.3553e-03,  ...,  3.1883e-02,\n",
      "          3.6870e-03,  2.3002e-02],\n",
      "        [ 1.5565e-03,  2.2101e-03,  1.3369e-02,  ..., -2.8294e-02,\n",
      "          3.9525e-03,  1.1983e-02],\n",
      "        [-5.5793e-03,  3.1928e-02, -2.3419e-02,  ..., -1.2321e-04,\n",
      "          1.1101e-02, -9.7526e-03]], requires_grad=True)\n",
      "[Oct 26, 09:01:35] #>>>> colbert doc ==\n",
      "[Oct 26, 09:01:35] #>>>>> D: torch.Size([180, 128]), tensor([[-0.2233, -0.0202, -0.3418,  ..., -0.1281, -0.1348, -0.2004],\n",
      "        [-0.0833,  0.1478, -0.3748,  ..., -0.2201, -0.2323, -0.0829],\n",
      "        [-0.0835,  0.1645, -0.5000,  ..., -0.1691, -0.2597, -0.0821],\n",
      "        ...,\n",
      "        [-0.1535,  0.1794, -0.4255,  ..., -0.1486, -0.3302, -0.1728],\n",
      "        [-0.1535,  0.1794, -0.4255,  ..., -0.1486, -0.3302, -0.1728],\n",
      "        [-0.1535,  0.1794, -0.4255,  ..., -0.1486, -0.3302, -0.1728]])\n",
      "[Oct 26, 09:01:36] [0] \t\t avg_doclen_est = 174.2857208251953 \t len(local_sample) = 7\n",
      "[Oct 26, 09:01:37] >> num_partitions_multiplier = 8, self.num_partitions = 256\n",
      "[Oct 26, 09:01:37] >> num_partitions limited to: self.num_partitions = 2\n",
      "[Oct 26, 09:01:37] [0] \t\t Creaing 2 partitions.\n",
      "[Oct 26, 09:01:37] [0] \t\t *Estimated* 1,220 embeddings.\n",
      "[Oct 26, 09:01:37] [0] \t\t #> Saving the indexing plan to /tmp/tmptrqmqrf1/output_dir/test_indexing/indexes/index_name/plan.json ..\n",
      "Sampling a subset of 512 / 1159 for training\n",
      "Clustering 512 points in 128D to 2 clusters, redo 1 times, 1 iterations\n",
      "  Preprocessing in 0.24 s\n",
      "[0.012, 0.013, 0.009, 0.01, 0.009, 0.01, 0.012, 0.015, 0.008, 0.011, 0.013, 0.011, 0.015, 0.008, 0.009, 0.011, 0.013, 0.01, 0.012, 0.011, 0.011, 0.017, 0.019, 0.011, 0.012, 0.015, 0.009, 0.013, 0.013, 0.016, 0.009, 0.01, 0.009, 0.015, 0.01, 0.013, 0.009, 0.01, 0.008, 0.01, 0.011, 0.013, 0.012, 0.012, 0.01, 0.011, 0.011, 0.01, 0.009, 0.012, 0.016, 0.014, 0.013, 0.012, 0.011, 0.017, 0.009, 0.012, 0.009, 0.01, 0.011, 0.009, 0.011, 0.01, 0.01, 0.009, 0.009, 0.009, 0.011, 0.012, 0.011, 0.011, 0.012, 0.014, 0.014, 0.011, 0.009, 0.01, 0.007, 0.008, 0.01, 0.007, 0.011, 0.012, 0.014, 0.011, 0.01, 0.013, 0.013, 0.011, 0.009, 0.012, 0.012, 0.012, 0.009, 0.011, 0.013, 0.016, 0.01, 0.009, 0.011, 0.013, 0.016, 0.011, 0.017, 0.011, 0.008, 0.011, 0.011, 0.012, 0.01, 0.011, 0.013, 0.011, 0.011, 0.01, 0.01, 0.012, 0.013, 0.014, 0.013, 0.01, 0.016, 0.009, 0.01, 0.013, 0.012, 0.012]\n",
      "[Oct 26, 09:01:42] #> Got bucket_cutoffs_quantiles = tensor([0.5000]) and bucket_weights_quantiles = tensor([0.2500, 0.7500])\n",
      "[Oct 26, 09:01:42] #> Got bucket_cutoffs = tensor([-0.0002]) and bucket_weights = tensor([-0.0092,  0.0088])\n",
      "[Oct 26, 09:01:42] avg_residual = 0.0114027950912714\n",
      "[Oct 26, 09:01:42] #> base_config.py from_path /tmp/tmptrqmqrf1/output_dir/test_indexing/indexes/index_name/metadata.json\n",
      "[Oct 26, 09:01:42] #> base_config.py from_path /tmp/tmptrqmqrf1/output_dir/test_indexing/indexes/index_name/plan.json\n",
      "[Oct 26, 09:01:42] #> base_config.py from_path args loaded! \n",
      "[Oct 26, 09:01:42] #> base_config.py from_path args replaced ! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 26, 09:01:42] [0] \t\t #> Encoding 7 passages..\n",
      "[Oct 26, 09:01:53] [0] \t\t #> Saving chunk 0: \t 7 passages and 1,220 embeddings. From #0 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:11, 11.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 26, 09:01:54] offset: 0\n",
      "[Oct 26, 09:01:54] chunk codes size(0): 1220\n",
      "[Oct 26, 09:01:54] codes size(0): 1220\n",
      "[Oct 26, 09:01:54] codes size(): torch.Size([1220])\n",
      "[Oct 26, 09:01:54] >>>>partition.size(0): 2\n",
      "[Oct 26, 09:01:54] >>>>num_partition: 2\n",
      "[Oct 26, 09:01:54] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Oct 26, 09:01:54] #> Building the emb2pid mapping..\n",
      "[Oct 26, 09:01:54] len(emb2pid) = 1220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 5493.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 26, 09:01:54] #> Saved optimized IVF to /tmp/tmptrqmqrf1/output_dir/test_indexing/indexes/index_name/ivf.pid.pt\n",
      "[Oct 26, 09:01:54] [0] \t\t #> Saving the indexing metadata to /tmp/tmptrqmqrf1/output_dir/test_indexing/indexes/index_name/metadata.json ..\n"
     ]
    }
   ],
   "source": [
    "with Run().context(RunConfig(root=args_dict['root'], experiment=args_dict['experiment'], nranks=args_dict['nranks'], amp=args_dict['amp'])):\n",
    "    colBERTConfig = ColBERTConfig(**args_dict)\n",
    "    create_directory(colBERTConfig.index_path_)\n",
    "    encode(colBERTConfig, collection_fn, None, None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b414c082",
   "metadata": {},
   "source": [
    "The resulting index files are in `output_dir/test_indexing/indexes/index_name/metadata.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfae0f75",
   "metadata": {},
   "source": [
    "## Search\n",
    "Next, we use the trained model and the index to search the collection, using queries from a TSV query file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9821799",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_fn = os.path.join(test_files_location, \"xorqa.train_ir_001pct_at_0_pct_queries_fornum.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40998e9e",
   "metadata": {},
   "source": [
    "Here are the search arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bdb5b7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = {\n",
    "                'root': output_dir,\n",
    "                'experiment': 'test_indexing' ,\n",
    "                'checkpoint': stage_2_student_model_fn,\n",
    "                'model_type': model_type,\n",
    "                'index_location': os.path.join(output_dir, 'test_indexing', 'indexes', 'index_name'),\n",
    "                'queries': queries_fn,\n",
    "                'bsize': 1,\n",
    "                'topK': 1,\n",
    "                'nway': 1,\n",
    "                'rank': 0,\n",
    "                'nranks': 1,\n",
    "                'amp': True,\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95e619e",
   "metadata": {},
   "source": [
    "Here we initalize and run the searcher:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a23cb798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 26, 09:09:49] #> base_config.py from_path /tmp/tmptrqmqrf1/output_dir/test_indexing/indexes/index_name/metadata.json\n",
      "[Oct 26, 09:09:49] #> base_config.py from_path args loaded! \n",
      "[Oct 26, 09:09:49] #> base_config.py from_path args replaced ! \n",
      "[Oct 26, 09:09:49] #> base_config.py load_from_checkpoint /tmp/tmptrqmqrf1/output_dir/test_training/2022-10/26/08.43.55/checkpoints/colbert\n",
      "[Oct 26, 09:09:49] #> base_config.py load_from_checkpoint /tmp/tmptrqmqrf1/output_dir/test_training/2022-10/26/08.43.55/checkpoints/colbert/artifact.metadata\n",
      "[Oct 26, 09:09:49] #> base_config.py from_path /tmp/tmptrqmqrf1/output_dir/test_training/2022-10/26/08.43.55/checkpoints/colbert/artifact.metadata\n",
      "[Oct 26, 09:09:49] #> base_config.py from_path args loaded! \n",
      "[Oct 26, 09:09:49] #>>>>> at ColBERT name (model type) : /tmp/tmptrqmqrf1/output_dir/test_training/2022-10/26/08.43.55/checkpoints/colbert\n",
      "[Oct 26, 09:09:49] #>>>>> at BaseColBERT name (model type) : /tmp/tmptrqmqrf1/output_dir/test_training/2022-10/26/08.43.55/checkpoints/colbert\n",
      "[Oct 26, 09:09:49] #> base_config.py load_from_checkpoint /tmp/tmptrqmqrf1/output_dir/test_training/2022-10/26/08.43.55/checkpoints/colbert\n",
      "[Oct 26, 09:09:49] #> base_config.py load_from_checkpoint /tmp/tmptrqmqrf1/output_dir/test_training/2022-10/26/08.43.55/checkpoints/colbert/artifact.metadata\n",
      "[Oct 26, 09:09:49] #> base_config.py from_path /tmp/tmptrqmqrf1/output_dir/test_training/2022-10/26/08.43.55/checkpoints/colbert/artifact.metadata\n",
      "[Oct 26, 09:09:49] #> base_config.py from_path args loaded! \n",
      "[Oct 26, 09:09:49] json file (get_colbert_from_pretrained): /tmp/tmptrqmqrf1/output_dir/test_training/2022-10/26/08.43.55/checkpoints/colbert/config.json\n",
      "[Oct 26, 09:09:49] factory model type: xlm-roberta-base\n",
      "[Oct 26, 09:10:02] json file (get_query_tokenizer): /tmp/tmptrqmqrf1/output_dir/test_training/2022-10/26/08.43.55/checkpoints/colbert/config.json\n",
      "[Oct 26, 09:10:02] get query model type: xlm-roberta-base\n",
      "[Oct 26, 09:10:06] json file (get_doc_tokenizer): /tmp/tmptrqmqrf1/output_dir/test_training/2022-10/26/08.43.55/checkpoints/colbert/config.json\n",
      "[Oct 26, 09:10:06] get doc model type: xlm-roberta-base\n",
      "[Oct 26, 09:10:08] #> Loading codec...\n",
      "[Oct 26, 09:10:08] #> base_config.py from_path /tmp/tmptrqmqrf1/output_dir/test_indexing/indexes/index_name/metadata.json\n",
      "[Oct 26, 09:10:08] #> base_config.py from_path args loaded! \n",
      "[Oct 26, 09:10:08] #> base_config.py from_path args replaced ! \n",
      "[Oct 26, 09:10:08] #> Loading IVF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/franzm/packages/minconda3/envs/primeqa_pyarrow/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 26, 09:10:09] Loading segmented_lookup_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Oct 26, 09:10:25] Loading filter_pids_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Oct 26, 09:10:38] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Oct 26, 09:10:43] #> Loading the queries from ../../../tests/resources/ir_dense/xorqa.train_ir_001pct_at_0_pct_queries_fornum.tsv ...\n",
      "[Oct 26, 09:10:43] #> Got 1 queries. All QIDs are unique.\n",
      "\n",
      "[Oct 26, 09:10:43] #> XMLR QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "[Oct 26, 09:10:43] #> Input: $ 중국에서 가장 오랜기간 왕위를 유지한 인물은 누구인가?, \t\t True, \t\t None\n",
      "[Oct 26, 09:10:43] #> Output IDs: torch.Size([32]), tensor([     0,   9748,  24120,   1180,  13968, 211059,  83639,  76826,  78363,\n",
      "         57104,    993, 161732,    697, 116932, 114150,     32,      2,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1])\n",
      "[Oct 26, 09:10:43] #> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "[Oct 26, 09:10:43] #>>>> colbert query ==\n",
      "[Oct 26, 09:10:43] #>>>>> input_ids: torch.Size([32]), tensor([     0,   9748,  24120,   1180,  13968, 211059,  83639,  76826,  78363,\n",
      "         57104,    993, 161732,    697, 116932, 114150,     32,      2,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/franzm/packages/minconda3/envs/primeqa_pyarrow/lib/python3.9/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 26, 09:10:47] #>>>> before linear query ==\n",
      "[Oct 26, 09:10:47] #>>>>> Q: torch.Size([32, 768]), tensor([[ 0.1762,  0.1015,  0.0653,  ..., -0.1249,  0.0495, -0.0164],\n",
      "        [ 0.0720,  0.0497,  0.0575,  ..., -0.0727, -0.1199, -0.0622],\n",
      "        [ 0.1116,  0.0483,  0.0107,  ...,  0.0009, -0.0022,  0.2009],\n",
      "        ...,\n",
      "        [ 0.1664,  0.0902, -0.0267,  ..., -0.2714, -0.0662,  0.0430],\n",
      "        [ 0.1664,  0.0902, -0.0267,  ..., -0.2714, -0.0662,  0.0430],\n",
      "        [ 0.1664,  0.0902, -0.0267,  ..., -0.2714, -0.0662,  0.0430]])\n",
      "[Oct 26, 09:10:47] #>>>>> self.linear query : Parameter containing:\n",
      "tensor([[-1.2609e-02, -3.6701e-03, -1.0775e-02,  ..., -1.3890e-02,\n",
      "          1.2242e-02, -8.2134e-03],\n",
      "        [-3.4918e-03,  3.9656e-03, -2.4894e-02,  ..., -2.6227e-02,\n",
      "          4.0927e-03, -8.0784e-03],\n",
      "        [-1.1166e-02, -2.3727e-05,  1.5029e-02,  ..., -1.1290e-02,\n",
      "         -9.3548e-03, -2.2198e-02],\n",
      "        ...,\n",
      "        [-1.0306e-02,  3.2584e-02,  3.3568e-03,  ...,  3.1891e-02,\n",
      "          3.6872e-03,  2.2994e-02],\n",
      "        [ 1.5555e-03,  2.2029e-03,  1.3376e-02,  ..., -2.8294e-02,\n",
      "          3.9596e-03,  1.1984e-02],\n",
      "        [-5.5736e-03,  3.1936e-02, -2.3412e-02,  ..., -1.2983e-04,\n",
      "          1.1101e-02, -9.7453e-03]], requires_grad=True)\n",
      "[Oct 26, 09:10:47] #>>>> colbert query ==\n",
      "[Oct 26, 09:10:47] #>>>>> Q: torch.Size([32, 128]), tensor([[-0.2493, -0.0526, -0.3429,  ..., -0.1253, -0.0886, -0.2121],\n",
      "        [-0.0317,  0.2806, -0.4828,  ..., -0.2698, -0.2687, -0.0353],\n",
      "        [ 0.0157,  0.3194, -0.4790,  ..., -0.3565, -0.2642, -0.0098],\n",
      "        ...,\n",
      "        [-0.1889, -0.0403, -0.3379,  ..., -0.1373, -0.0523, -0.2145],\n",
      "        [-0.1889, -0.0403, -0.3379,  ..., -0.1373, -0.0523, -0.2145],\n",
      "        [-0.1889, -0.0403, -0.3379,  ..., -0.1373, -0.0523, -0.2145]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.14it/s]\n"
     ]
    }
   ],
   "source": [
    "with Run().context(RunConfig(root=args_dict['root'], experiment=args_dict['experiment'], nranks=args_dict['nranks'], amp=args_dict['amp'])):\n",
    "    colBERTConfig = ColBERTConfig(**args_dict)\n",
    "    searcher = Searcher(args_dict['index_location'], checkpoint=args_dict['checkpoint'], config=colBERTConfig)\n",
    "    rankings = searcher.search_all(args_dict['queries'], args_dict['topK'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ceac32",
   "metadata": {},
   "source": [
    "Here is the search result for our query, containing [query_id, document_id, rank, score]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd8e1b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-7239279093922981232, 2, 1, 30.695838928222656)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rankings.flat_ranking[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a888747c",
   "metadata": {},
   "source": [
    "Here is the text of the query record, contataing the ID and text of the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6027f452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7239279093922981232\t중국에서 가장 오랜기간 왕위를 유지한 인물은 누구인가?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(queries_fn, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        if str(rankings.flat_ranking[0][0]) == line.split()[0]:\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf0a038",
   "metadata": {},
   "source": [
    "English translation: `Who maintained the throne for the longest time in China?`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1408c62",
   "metadata": {},
   "source": [
    "Here is the top retrieved document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89133f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\tYao. The Bamboo Annals says that when Emperor Zhuanxu died, a descendent of Shennong named ShuQe raised a disturbance, but was destroyed by the prince of Sin, who was Ku (GaoXin), a descendant of HuangDi, who then ascended to the throne. In the 45th year, Ku designated the prince of Tang (唐) (his son Yao) as his successor, however upon his death in the 63rd year, his elder son Zhi then took the throne instead, ruling nine years before being deposed and replaced by Yao. Emperor Zhi Di Zhì () or simply Zhì, was a mythological emperor of ancient China.\tEmperor Zhi\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(collection_fn, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        if str(rankings.flat_ranking[0][1]) == line.split()[0]:\n",
    "            print(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
