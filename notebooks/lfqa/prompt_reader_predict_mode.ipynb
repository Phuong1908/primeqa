{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
      "{\"time\":\"2023-02-03 12:55:30,072\", \"name\": \"faiss.loader\", \"level\": \"INFO\", \"message\": \"Loading faiss.\"}\n",
      "{\"time\":\"2023-02-03 12:55:30,155\", \"name\": \"faiss.loader\", \"level\": \"INFO\", \"message\": \"Successfully loaded faiss.\"}\n",
      "[Feb 03, 12:55:32] #> base_config.py from_path /dccstor/mabornea1/kilt-wikipedia-test/colbert_ir/kilt_wikipedia_eli5_dev_exp/indexes//kilt_wikipedia_eli5_dev_indname/metadata.json\n",
      "[Feb 03, 12:55:32] #> base_config.py from_path args loaded! \n",
      "[Feb 03, 12:55:32] #> base_config.py from_path args replaced ! \n",
      "[Feb 03, 12:55:40] #>>>>> at ColBERT name (model type) : /dccstor/colbert-ir/franzm/experiments/oct2_7_12_1.5e-06/none/2022-10/09/15.21.39/checkpoints/colbert.dnn.batch_91287.model\n",
      "[Feb 03, 12:55:40] #>>>>> at BaseColBERT name (model type) : /dccstor/colbert-ir/franzm/experiments/oct2_7_12_1.5e-06/none/2022-10/09/15.21.39/checkpoints/colbert.dnn.batch_91287.model\n",
      "[Feb 03, 12:55:54] factory model type: xlm-roberta-large\n",
      "[Feb 03, 12:56:16] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Feb 03, 12:56:24] get query model type: xlm-roberta-large\n",
      "[Feb 03, 12:56:25] get doc model type: xlm-roberta-large\n",
      "[Feb 03, 12:56:26] #> Loading codec...\n",
      "[Feb 03, 12:56:26] #> base_config.py from_path /dccstor/mabornea1/kilt-wikipedia-test/colbert_ir/kilt_wikipedia_eli5_dev_exp/indexes//kilt_wikipedia_eli5_dev_indname/metadata.json\n",
      "[Feb 03, 12:56:26] #> base_config.py from_path args loaded! \n",
      "[Feb 03, 12:56:26] #> base_config.py from_path args replaced ! \n",
      "[Feb 03, 12:56:26] #> Loading IVF...\n",
      "[Feb 03, 12:56:26] Loading segmented_lookup_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/mabornea1/miniconda3/envs/prompt_reader/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 03, 12:56:27] Loading filter_pids_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Feb 03, 12:56:28] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14635it [00:00, 198680.84it/s]\n"
     ]
    }
   ],
   "source": [
    "from primeqa.components.retriever.dense import ColBERTRetriever\n",
    "from primeqa.components.reader.prompt import PromptGPTReader\n",
    "from primeqa.pipelines.qa_pipeline import QAPipeline\n",
    "import json\n",
    "\n",
    "index_root = \"/dccstor/mabornea1/kilt-wikipedia-test/colbert_ir/kilt_wikipedia_eli5_dev_exp/indexes/\"\n",
    "index_name = \"kilt_wikipedia_eli5_dev_indname\"\n",
    "collection = \"/dccstor/mabornea1/kilt-wikipedia-test/passages/kilt_knowledgesource_eli5_dev.tsv\"\n",
    "\n",
    "\n",
    "retriever = ColBERTRetriever(index_root = index_root, \n",
    "                                     index_name = index_name, \n",
    "                                     collection = collection, \n",
    "                                     max_num_documents = 3)\n",
    "retriever.load()\n",
    "\n",
    "reader = PromptGPTReader(api_key=\"\")\n",
    "reader.load()\n",
    "\n",
    "pipeline = QAPipeline(retriever, reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 03, 12:56:40] #> XMLR QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "[Feb 03, 12:56:40] #> Input: $ What causes the trail behind jets at high altitude?, \t\t True, \t\t None\n",
      "[Feb 03, 12:56:40] #> Output IDs: torch.Size([32]), tensor([     0,   9748,   4865, 113660,     70, 141037,  50155,     55,    933,\n",
      "            99,  11192,    144,  35810,     32,      2,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1])\n",
      "[Feb 03, 12:56:40] #> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "[Feb 03, 12:56:40] #>>>> colbert query ==\n",
      "[Feb 03, 12:56:40] #>>>>> input_ids: torch.Size([32]), tensor([     0,   9748,   4865, 113660,     70, 141037,  50155,     55,    933,\n",
      "            99,  11192,    144,  35810,     32,      2,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/mabornea1/miniconda3/envs/prompt_reader/lib/python3.9/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 03, 12:56:40] #>>>> before linear query ==\n",
      "[Feb 03, 12:56:40] #>>>>> Q: torch.Size([32, 1024]), tensor([[-0.9069, -0.0403,  1.6935,  ..., -2.0556, -0.3505,  0.3143],\n",
      "        [-0.6235, -0.2982,  0.1217,  ...,  0.0441, -1.5926, -0.2264],\n",
      "        [-0.4985, -0.5053, -0.0043,  ...,  0.1960, -1.3059, -0.4401],\n",
      "        ...,\n",
      "        [-1.5241, -0.1893,  0.4258,  ...,  0.2577, -1.6957, -0.0245],\n",
      "        [-1.5241, -0.1893,  0.4258,  ...,  0.2577, -1.6957, -0.0245],\n",
      "        [-1.5241, -0.1893,  0.4258,  ...,  0.2577, -1.6957, -0.0245]])\n",
      "[Feb 03, 12:56:40] #>>>>> self.linear query : Parameter containing:\n",
      "tensor([[-0.0301, -0.0307, -0.0115,  ..., -0.0231, -0.0023, -0.0216],\n",
      "        [ 0.0053,  0.0023, -0.0308,  ...,  0.0108,  0.0011,  0.0201],\n",
      "        [-0.0220,  0.0370,  0.0339,  ..., -0.0023, -0.0172,  0.0244],\n",
      "        ...,\n",
      "        [ 0.0222,  0.0115, -0.0246,  ...,  0.0389, -0.0034, -0.0165],\n",
      "        [-0.0146,  0.0392,  0.0131,  ..., -0.0055,  0.0219, -0.0368],\n",
      "        [ 0.0071,  0.0256, -0.0346,  ...,  0.0322,  0.0370,  0.0437]],\n",
      "       requires_grad=True)\n",
      "[Feb 03, 12:56:40] #>>>> colbert query ==\n",
      "[Feb 03, 12:56:40] #>>>>> Q: torch.Size([32, 128]), tensor([[-0.6141, -0.0460, -0.0425,  ..., -0.3479, -0.5222, -0.3503],\n",
      "        [ 0.1191, -1.1724,  0.6583,  ..., -0.1719, -0.6755,  0.7421],\n",
      "        [ 0.0661, -1.2071,  0.6980,  ..., -0.3364, -0.7605,  0.8986],\n",
      "        ...,\n",
      "        [-0.2243, -0.7942,  0.3333,  ...,  0.1308, -0.7902,  0.1236],\n",
      "        [-0.2243, -0.7942,  0.3333,  ...,  0.1308, -0.7902,  0.1236],\n",
      "        [-0.2243, -0.7942,  0.3333,  ...,  0.1308, -0.7902,  0.1236]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 40.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We do not currently have a prompt\n",
      "[\n",
      "    {\n",
      "        \"example_id\": 0,\n",
      "        \"text\": \"This is a placeholder and we do not call the API\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "questions = [\"What causes the trail behind jets at high altitude?\"]\n",
    "answers = pipeline.run(questions)\n",
    "print(json.dumps(answers, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt_reader",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c5d039775dffd761dc362a240b88aab365529f2df8e87d6e6e9eecd3e8d89fd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
