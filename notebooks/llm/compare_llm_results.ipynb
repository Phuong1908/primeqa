{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read Directory of experiments to compare\n",
    "2. Compute Rouge for each experiment\n",
    "3. Keep best score\n",
    "4. Print best in a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_best_params(baseline_dir):\n",
    "    best_rouge = 0\n",
    "    best_df = None\n",
    "    best_name = \"\"\n",
    "\n",
    "    baseline_files = glob.glob(baseline_dir)\n",
    "\n",
    "    for baseline_file in baseline_files:\n",
    "        baseline_df = pd.read_json(baseline_file, lines=True)\n",
    "        score = baseline_df['rouge'].mean()\n",
    "        if score > best_rouge:\n",
    "            best_rouge = score\n",
    "            best_df = baseline_df\n",
    "            best_name = baseline_file\n",
    "\n",
    "    return best_rouge, best_name, best_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_rouge_table(dataset):\n",
    "    llms = [\"google-flan-t5-xxl\", \"bigscience-bloomz\", \"bigscience-bloom\", \"eleutherai-gpt-neox-20b\"]\n",
    "\n",
    "    baseline_dir = \"/dccstor/srosent2/generative/baseline_llms/\" + dataset\n",
    "    best_examples = []\n",
    "\n",
    "    print(\"experiment|rouge|passages|n-shot|top p|top k|temperature|min length|max length\")\n",
    "    for llm in llms: \n",
    "        baseline_fnames = baseline_dir + \"/\" + llm + \"/*/predictions*.json\"\n",
    "\n",
    "        best_rouge, best_name, best_df = get_best_params(baseline_fnames)\n",
    "\n",
    "        params = best_name[len(baseline_dir + llm + \"/\"):best_name.rindex(\"/\")].split(\"-\")\n",
    "\n",
    "        print_output = llm + \"|\" + str(best_rouge) + \"|\"\n",
    "        print_output += params[1].split(\"_\")[1] + \"|\"\n",
    "        print_output += params[2][0] + \"|\"\n",
    "        pktemp = params[3].split(\"_\")\n",
    "        print_output += pktemp[0] + \"|\" + pktemp[1] + \"|\" + pktemp[2] + \"|\"\n",
    "        minmax = params[4].split(\"_\")\n",
    "        print_output += minmax[1] + \"|\" + minmax[2]\n",
    "        print(print_output)\n",
    "        for i, row in best_df.iterrows():\n",
    "            best_examples.append([llm, str(row['id']), row['question'], row['text'], str(row['rouge'])])\n",
    "    return best_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment|rouge|passages|n-shot|top p|top k|temperature|min length|max length\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open('/dccstor/srosent2/generative/baseline_llms/NQ/best.csv', 'w') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerows(get_rouge_table(\"NQ\"))\n",
    "with open('/dccstor/srosent2/generative/baseline_llms/ELI5/best.csv', 'w') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerows(get_rouge_table(\"ELI5\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "experiment|rouge|passages|n-shot|top p|top k|temperature|min length|max length\n",
    "google-flan-t5-xxl|0.25508641580641983|True|0|0.25|100|0.5|200|1024\n",
    "bigscience-bloomz|0.2541450173173494|True|0|1.0|100|0.75|100|1024\n",
    "bigscience-bloom|0.22344075436693506|True|0|0.75|100|0.5|50|1024\n",
    "eleutherai-gpt-neox-20b|0.16836206596932884|True|0|0.5|100|1.0|0|1024"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "primeqa4.24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d32919132f66e210a1b695050b8f424e37551142a4189348e2af6a594afe21a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
