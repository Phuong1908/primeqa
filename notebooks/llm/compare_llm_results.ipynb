{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read Directory of experiments to compare\n",
    "2. Compute Rouge for each experiment\n",
    "3. Keep best score\n",
    "4. Print best in a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "rouge = Rouge(exclusive=False)\n",
    "\n",
    "def rougel_score(prediction, ground_truth):\n",
    "    # no normalization\n",
    "    try:\n",
    "        scores = rouge.get_scores(prediction, ground_truth, avg=True)\n",
    "    except ValueError:  # \"Hypothesis is empty.\"\n",
    "        return 0.0, 0.0\n",
    "    return scores[\"rouge-l\"][\"f\"], scores[\"rouge-1\"][\"f\"]\n",
    "\n",
    "import datasets\n",
    "hf_rouge = datasets.load_metric('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gold Data\n",
    "\n",
    "import json\n",
    "ELI5 = {}\n",
    "NQ = {}\n",
    "\n",
    "with open(\"/dccstor/srosent2/generative/eli5-sample/5-ELI5-train-examples-for-evaluation.jsonl\",'r') as f:\n",
    "    for line in f.readlines():\n",
    "        data = json.loads(line)\n",
    "        ELI5[data['id']] = data\n",
    "with open(\"/dccstor/srosent2/generative/appen/NQ_sample.json\",'r') as f:\n",
    "    for line in f.readlines():\n",
    "        data = json.loads(line)\n",
    "        NQ[data['id']] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_best_params(baseline_dir, ref):\n",
    "    best_rouge = 0\n",
    "    best_df = None\n",
    "    best_name = \"\"\n",
    "\n",
    "    baseline_files = glob.glob(baseline_dir)\n",
    "\n",
    "    for baseline_file in baseline_files:\n",
    "        baseline_df = pd.read_json(baseline_file, lines=True, dtype=str)\n",
    "        baseline_df['hf_rouge'] = 0.0\n",
    "\n",
    "        for i, row in baseline_df.iterrows():\n",
    "            hf_rouge_score = hf_rouge.compute(predictions=[row[\"text\"]],references=[ref[row[\"id\"]]['output'][0]['answer']])\n",
    "            kilt_rouge_score, _ = rougel_score(row[\"text\"],ref[row[\"id\"]]['output'][0]['answer'])\n",
    "            baseline_df.loc[i, 'hf_rouge'] = hf_rouge_score[\"rougeLsum\"].mid.fmeasure\n",
    "            baseline_df.loc[i, 'kilt_rouge'] = kilt_rouge_score\n",
    "            \n",
    "        score = baseline_df['hf_rouge'].mean()\n",
    "        if score > best_rouge:\n",
    "            best_rouge = score\n",
    "            best_df = baseline_df\n",
    "            best_name = baseline_file\n",
    "\n",
    "    return best_rouge, best_name, best_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_rouge_table(dataset, ref):\n",
    "    llms = [\"google-flan-t5-xxl\", \"bigscience-bloomz\", \"bigscience-bloom\", \"eleutherai-gpt-neox-20b\"]\n",
    "\n",
    "    baseline_dir = \"/dccstor/srosent2/generative/baseline_llms/\" + dataset\n",
    "    best_examples = []\n",
    "\n",
    "    print(\"experiment|rouge|passages|n-shot|top p|top k|temperature|min length|max length\")\n",
    "    for llm in llms: \n",
    "        baseline_fnames = baseline_dir + \"/\" + llm + \"/*/predictions*.json\"\n",
    "\n",
    "        best_rouge, best_name, best_df = get_best_params(baseline_fnames, ref)\n",
    "\n",
    "        params = best_name[len(baseline_dir + llm + \"/\"):best_name.rindex(\"/\")].split(\"-\")\n",
    "\n",
    "        print_output = llm + \"|\" + str(best_rouge) + \"|\"\n",
    "        print_output += params[1].split(\"_\")[1] + \"|\"\n",
    "        print_output += params[2][0] + \"|\"\n",
    "        pktemp = params[3].split(\"_\")\n",
    "        print_output += pktemp[0] + \"|\" + pktemp[1] + \"|\" + pktemp[2] + \"|\"\n",
    "        minmax = params[4].split(\"_\")\n",
    "        print_output += minmax[1] + \"|\" + minmax[2]\n",
    "        print(print_output)\n",
    "        for i, row in best_df.iterrows():\n",
    "            best_examples.append([llm, str(row['id']), row['question'], row['text'], str(row['rouge']), str(row['hf_rouge']), str(row['kilt_rouge'])])\n",
    "    return best_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment|rouge|passages|n-shot|top p|top k|temperature|min length|max length\n",
      "google-flan-t5-xxl|0.5212973087322894|True|0|0.5|100|1.0|50|1024\n",
      "bigscience-bloomz|0.44545249836421624|True|0|1.0|100|0.5|50|1024\n",
      "bigscience-bloom|0.14787958830481615|True|0|0.25|100|1.0|0|1024\n",
      "eleutherai-gpt-neox-20b|0.06925018935010438|True|0|1.0|100|0.75|100|1024\n",
      "experiment|rouge|passages|n-shot|top p|top k|temperature|min length|max length\n",
      "google-flan-t5-xxl|0.19292226292226294|True|0|1.0|100|1.0|50|1024\n",
      "bigscience-bloomz|0.22076311751313557|True|0|1.0|100|0.5|50|1024\n",
      "bigscience-bloom|0.1049867513622758|True|0|0.5|100|1.0|50|1024\n",
      "eleutherai-gpt-neox-20b|0.14495064747492892|True|0|0.75|100|1.0|0|1024\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open('/dccstor/srosent2/generative/baseline_llms/NQ/best_updated.csv', 'w') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerows(get_rouge_table(\"NQ\", NQ))\n",
    "with open('/dccstor/srosent2/generative/baseline_llms/ELI5/best_updated.csv', 'w') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerows(get_rouge_table(\"ELI5\", ELI5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "experiment|rouge|passages|n-shot|top p|top k|temperature|min length|max length\n",
    "google-flan-t5-xxl|0.25508641580641983|True|0|0.25|100|0.5|200|1024\n",
    "bigscience-bloomz|0.2541450173173494|True|0|1.0|100|0.75|100|1024\n",
    "bigscience-bloom|0.22344075436693506|True|0|0.75|100|0.5|50|1024\n",
    "eleutherai-gpt-neox-20b|0.16836206596932884|True|0|0.5|100|1.0|0|1024"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "primeqa4.24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d32919132f66e210a1b695050b8f424e37551142a4189348e2af6a594afe21a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
