{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read Directory of experiments to compare\n",
    "2. Compute Rouge for each experiment\n",
    "3. Keep best score\n",
    "4. Print best in a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_best_params(baseline_dir):\n",
    "    best_rouge = 0\n",
    "    best_df = None\n",
    "    best_name = \"\"\n",
    "\n",
    "    baseline_files = glob.glob(baseline_dir)\n",
    "\n",
    "    for baseline_file in baseline_files:\n",
    "        baseline_df = pd.read_json(baseline_file, lines=True)\n",
    "        score = baseline_df['rouge'].mean()\n",
    "        if score > best_rouge:\n",
    "            best_rouge = score\n",
    "            best_df = baseline_df\n",
    "            best_name = baseline_file\n",
    "\n",
    "    return best_rouge, best_name, best_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment|rouge|passages|n-shot|top p|top k|temperature|min length|max length\n",
      "google-flan-t5-xxl|0.2383960697896196|True|0|0.25|100|0.75|200|1024\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "substring not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m baseline_fnames \u001b[39m=\u001b[39m baseline_dir \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m llm \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/*/predictions*.json\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m best_rouge, best_name, best_df \u001b[39m=\u001b[39m get_best_params(baseline_fnames)\n\u001b[0;32m---> 11\u001b[0m params \u001b[39m=\u001b[39m best_name[\u001b[39mlen\u001b[39m(baseline_dir \u001b[39m+\u001b[39m llm \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m):best_name\u001b[39m.\u001b[39;49mrindex(\u001b[39m\"\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m\"\u001b[39;49m)]\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m print_output \u001b[39m=\u001b[39m llm \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m|\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(best_rouge) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m|\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m print_output \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m params[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m|\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: substring not found"
     ]
    }
   ],
   "source": [
    "llms = [\"google-flan-t5-xxl\", \"bigscience-bloomz\", \"bigscience-bloom\", \"eleutherai-gpt-neox-20b\"]\n",
    "\n",
    "baseline_dir = \"/dccstor/srosent2/generative/baseline_llms/ELI5/\"\n",
    "\n",
    "print(\"experiment|rouge|passages|n-shot|top p|top k|temperature|min length|max length\")\n",
    "for llm in llms: \n",
    "    baseline_fnames = baseline_dir + \"/\" + llm + \"/*/predictions*.json\"\n",
    "\n",
    "    best_rouge, best_name, best_df = get_best_params(baseline_fnames)\n",
    "\n",
    "    params = best_name[len(baseline_dir + llm + \"/\"):best_name.rindex(\"/\")].split(\"-\")\n",
    "\n",
    "    print_output = llm + \"|\" + str(best_rouge) + \"|\"\n",
    "    print_output += params[1].split(\"_\")[1] + \"|\"\n",
    "    print_output += params[2][0] + \"|\"\n",
    "    pktemp = params[3].split(\"_\")\n",
    "    print_output += pktemp[0][6:] + \"|\" + pktemp[1] + \"|\" + pktemp[2] + \"|\"\n",
    "    minmax = params[4].split(\"_\")\n",
    "    print_output += minmax[1] + \"|\" + minmax[2]\n",
    "    print(print_output)\n",
    "    # best_df.sample(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "experiment|rouge|passages|n-shot|top p|top k|temperature|min length|max length\n",
    "google-flan-t5-xxl|0.25508641580641983|True|0|0.25|100|0.5|200|1024\n",
    "bigscience-bloomz|0.2541450173173494|True|0|1.0|100|0.75|100|1024\n",
    "bigscience-bloom|0.22344075436693506|True|0|0.75|100|0.5|50|1024\n",
    "eleutherai-gpt-neox-20b|0.16836206596932884|True|0|0.5|100|1.0|0|1024"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "primeqa4.24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d32919132f66e210a1b695050b8f424e37551142a4189348e2af6a594afe21a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
